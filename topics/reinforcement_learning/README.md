#  Reinforcement Learning


- Model-Free
- Model-Based
- Robotics
- Games
- Exploration vs. Exploitation
- Inverse RL
- Multi-Agent
- Hierarchical Learning
- Transfer Learning
- ...

| Date             | Title                                                                                                          | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ---------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 7/4/23<br>ICML23 | **Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openai/weak-to-strong)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Posts-yellow?logo=rss)](https://mp.weixin.qq.com/s/f6YW-CxnLhnfMWTLg4M4Cw)</div><div style='width:150px;'>[![Note](https://img.shields.io/badge/Note-Read-blue?logo=dependabot)](summary/2024-03/2403.18349.md)</div> |
|                  |                                                                                                                |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

###  Policy Gradient

| Date             | Title                                                                                         | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| ---------------- | --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 1992             | **Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://link.springer.com/content/pdf/10.1023/A:1022672621406.pdf)</div>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| 2001             | **Infinite-Horizon Policy-Gradient Estimation**                                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1106.0665)</div>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| 2008             | **Reinforcement learning of motor skills with policy gradients**                              | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://pdf.sciencedirectassets.com/271125/1-s2.0-S0893608008X00048/1-s2.0-S0893608008000701/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEEoaCXVzLWVhc3QtMSJHMEUCIQCrbDjutZJaF9fpLTATtd9MWVVsfJ8xs%2BbfmIu8qJyYvgIgCYQuhXiInU7Ly0ly1slo%2BmEfj9gglg8B6%2FV6YMOZdbYqvAUIg%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDEfbT%2FdTiKy%2BKyypqCqQBSxE8DnNZ6t2mW0SakI9RYNLJX1DIjyA%2BrPxINHAPPzJZvsf%2BwalCEelZrXmAgFvjo%2FPZF9pvsKTRcrWi3%2BajrAAQqN9g86rT0zaPr4q2HHw0EbgsdY35hB38JFyTmy6EzecRdb%2FwaQFjjt5NtI7kpoAJ8iNA9KtxIYMaj31hK%2FC9oPc2Fhn2fdmCmnzfUD1526rgP2kCgDOLJwwody66sEP14EzJfM3Djonx71rHqkquRBp8qdEAVkHEpjCnXfvK%2BP%2BrfaLVpWVxLmFDvEpZ1dmHclMY%2BfdIyv%2FrS4vErk7k8SLlsgQ0zxoYwQTCQ4duHvYYqCSqyEhsAvk2TAhY4kWaJlBEmj023leISNkyITaYKpjYilqnKBl1057BOnQQfBntXg7D1tsokreb8enepAYBerNS1QnOC%2B%2BRA3NrewHPQCrp%2Bc0CZWsPsFAKBJCeAp3aANJpPIHIu66bId4iEGANiczWjs5p3fiUYYlV%2B01y4%2FNXsd6OaFiC6oMW3Y463TEZHoVPw7eXJNld8kpu2Q%2F%2FYrhCsrIelAi%2FMwYhCNwmIQOK5ZZqHRxsGG%2FZjcCegdGzCL9oBY9myvIc7TcHCOG72eeW4GffdGBs7Fu59UCLmv2xAuI8177pqB2UtlWpf%2FYT7hRJemLfXaPOkZ8pLCakWFWi4lDAG3GSKWGzeW3r3FIywCPalKYGnuF4TSmMRzSje1Y%2BRIka0ggdEVBS6mWW%2BLAfOXBSWhm3bBC%2By4ZEmtzWlVFDFpyX%2FFmFlv%2Bya2fG43VDK5e3h7VXfz8UTjx8IJIla1vIw7Or1uoqQ8w0qovRyx9susqwHcgIA2J7ixR6vLQh7K7p9BGHfC1f6WRW9KttVeUvvqsep1ut1BrMLPw7LAGOrEB3ZV1myrA3dyvuBBCgTMXPAtHqB7k6X4jaXLWArTc3BAyOF1uv1dlc1lCroUM1nVmgKjbIYDtMrZzDCQPgiW33GiZ8r28KucdYW0crmLaqfVIcxZNNxcQGh3PTzu23DkKWj%2FbiQrNscaMJ5K%2BsD2n6M6%2Fm3YsYykBkvbKnMyimQdPeWjEXJW9RyDc%2BaHazkSKR%2BXxXvYTEtgVJrMeWvOfExQJhJzi9DkQzxeAeA%2FW4i7C&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240414T030905Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYXBV4DVVQ%2F20240414%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=2e86fdf7c56957f3ce846c5ef03c04361f437341959232fb38c364f5b131e96b&hash=3c7decb8493528a387c05f38864d64fc7d47c37794a8b17f69b0c09f1b3d006b&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0893608008000701&tid=spdf-827c9808-ba3c-416c-b189-5356686ce35f&sid=83274a559230084f9e38e321bd2ccbccaba3gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0d1458565551580b0606&rr=87407ebbaf413273&cc=kr)</div> |
| ICML<br>2013<br> | **Guided Policy Search**                                                                      | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf)</div>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ICML<br>2015<br> | **Trust Region Policy Optimization**                                                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1502.05477)</div>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| 2017             | **Proximal Policy Optimization Algorithms**                                                   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1707.06347)</div>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
|                  |                                                                                               |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |


### Actor-Critic

| Date                | Title                                                                              | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                    |
| ------------------- | ---------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| NeurIPS<br>1999<br> | **Policy Gradient Methods for Reinforcement Learning with Function Approximation** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://papers.nips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)</div> |
| ICML<br>2016<br>    | **Asynchronous Methods for Deep Reinforcement Learning**                           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1602.01783)</div>                                                              |
| ICLR<br>2016<br>    | **High-Dimensional Continuous Control Using Generalized Advantage Estimation**     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1506.02438)</div>                                                              |
| ICLR<br>2017        | **Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic**             | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1611.02247)</div>                                                              |
|                     |                                                                                    |                                                                                                                                                                               |                                                                                                                                                                                                          |



### Exploration

| Date                     | Title                                                                                         | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                                  |
| ------------------------ | --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| NeurIPS<br>2016<br>      | **Unifying Count-Based Exploration and Intrinsic Motivation**                                 | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1606.01868)</div>                                                                                                                                                            |
| NeurIPS<br>2011          | **An Empirical Evaluation of Thompson Sampling**                                              | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://proceedings.neurips.cc/paper_files/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf)</div>                                                                                       |
| NeurIPS<br>2017          | **# Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning**         | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1611.04717)</div>                                                                                                                                                            |
| NeurIPS<br>2017          | **EX2: Exploration with Exemplar Models for Deep Reinforcement Learning**                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1703.01260)</div>                                                                                                                                                            |
| ICLR<br>2019             | **Exploration by Random Network Distillation**                                                | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1810.12894)</div>                                                                                                                                                            |
| NeurIPS<br>2016          | **Deep Exploration via Bootstrapped DQN**                                                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1602.04621)</div>                                                                                                                                                            |
| NeurIPS<br>2016          | **VIME: Variational Information Maximizing Exploration**                                      | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1605.09674)</div>                                                                                                                                                            |
| 1992                     | **A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://sferics.idsia.ch/pub/juergen/curiositysab.pdf)</div>                                                                                                                                       |
| ICLR<br>2016             | **Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models**           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1507.00814)</div>                                                                                                                                                            |
| NeurIPS<br>2018          | **Visual Reinforcement Learning with Imagined Goals**                                         | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1807.04742)</div>                                                                                                                                                            |
| ICML<br>2020             | **Skew-Fit: State-Covering Self-Supervised Reinforcement Learning**                           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1903.03698)</div>                                                                                                                                                            |
| Jun/2019                 | **Efficient Exploration via State Marginal Matching**                                         | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1906.05274)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/state-marginal-matching)</div> |
| ICML<br>2019             | **Provably Efficient Maximum Entropy Exploration**                                            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1812.02690)</div>                                                                                                                                                            |
| Jun/2018                 | **Unsupervised Meta-Learning for Reinforcement Learning**                                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1806.04640)</div>                                                                                                                                                            |
| ICLR<br>2019             | **Diversity is All You Need: Learning Skills without a Reward Function**                      | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1802.06070)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/diayn/)</div>                  |
| ICLR<br>workshop<br>2017 | **Variational Intrinsic Control**                                                             | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1611.07507)</div>                                                                                                                                                            |
|                          |                                                                                               |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                        |

### Offline RL
| Date             | Title                                                                                                          | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ---------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 7/4/23<br>ICML23 | **Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openai/weak-to-strong)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Posts-yellow?logo=rss)](https://mp.weixin.qq.com/s/f6YW-CxnLhnfMWTLg4M4Cw)</div><div style='width:150px;'>[![Note](https://img.shields.io/badge/Note-Read-blue?logo=dependabot)](summary/2024-03/2403.18349.md)</div> |
|                  |                                                                                                                |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
