# Reinforcement Learning


**üìãCatalogue**
* [Imitation Learning](#imitation-learning)
* [Policy Gradient](#policy-gradient)
* [Actor-Critic](#actor-critic)
* [Q-Learning](#q-learning)
* [Planning](#planning)
* [Model-Based](#model-based)
* [Exploration](#exploration)
* [Offline RL](#offline-rl)
	*  [Importance Sampling](#importance-sampling)
* [Inference](#inference)
	* [Variational Inference](#variational-inference)
* [Inverse RL](#inverse-rl)
* [Sequence Model](#sequence-model)
* [Transfer Learning](#transfer-learning)
	* [Multi-Task Learning](#multi-task-learning)
	* [Meta-Learning](#meta-learning)


**üî¨Resources**
* [Books](#books)
* [Researchers](#researchers)
* [Library](./rl_knowledge_library.md)


##  Imitation Learning

| Date            | Title                                                                                                        | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Links                                                                                                                                                                                                                                                                                          |
| --------------- | ------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| AISTATS<br>2011 | **A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning**                 | <sub>Propose **DAGGER**(Dataset Aggregation). **DAGGER** proceeds by collecting a dataset at each iteration under the current policy and trains the next policy under the aggregate of all collected datasets. ‚≠ê\|üå∏\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|üòâ</sub>                                                                                                                                                                                                                                                                                                       | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1011.0686)</div>                                                                                                                                                     |
| ICRA<br>2018    | **Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-To-End Learning from Demonstration** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                     | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1707.02920)</div>                                                                                                                                                    |
| NeurIPS<br>2019 | **Causal Confusion in Imitation Learning**                                                                   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                     | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1905.11979)</div>                                                                                                                                                    |
| RSS<br>2023     | **Diffusion Policy: Visuomotor Policy Learning via Action Diffusion**                                        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                     | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2303.04137)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://diffusion-policy.cs.columbia.edu/)</div>     |
| RSS<br>2023     | **Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware**                                       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                     | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2304.13705)</div>                                                                                                                                                    |
| 2022            | **RT-1: Robotics Transformer for Real-World Control at Scale**                                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                     | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2212.06817)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robotics-transformer1.github.io/)</div>      |
| CoRL<br>2019    | **Learning Latent Plans from Play**                                                                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                     | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1903.01973)</div>                                                                                                                                                    |
| RSS<br>2019     | **Unsupervised Visuomotor Control through Distributional Planning Networks**                                 | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                     | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1902.05542)</div>                                                                                                                                                    |
| ICRA<br>2023    | **GNM: A General Navigation Model to Drive Any Robot**                                                       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                     | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2210.03370)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/drive-any-robot)</div> |
| RSS<br>2024     | **3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations**              | <sub>Based on the **Diffusion Policy**, the paper proposed **3D Diffusion Policy**, a visual imitation learning algorithm, which uses **3D representation** with diffusion policies. Specifically, the DP3 perceives the environment through single-view point clouds. And then these are transformed into compact 3D representations by a lightweight MLP encoder. Subsequently, DP3 generates **actions** conditioning on these **3D representations** and the **robot‚Äôs states**, using a diffusion-based backbone. üí´\|üå∑\|‚òÑÔ∏è\|üëçüèΩ\|ü§®</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2403.03954)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://3d-diffusion-policy.github.io/)</div>        |
|                 |                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                |


## Policy Gradient

| Date             | Title                                                                                         | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                       |
| ---------------- | --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1992             | **Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://link.springer.com/content/pdf/10.1023/A:1022672621406.pdf)</div>                                                |
| 2001             | **Infinite-Horizon Policy-Gradient Estimation**                                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1106.0665)</div>                                                                                  |
| 2008             | **Reinforcement learning of motor skills with policy gradients**                              | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Publications/Neural-Netw-2008-21-682_4867[0].pdf)</div> |
| ICML<br>2013<br> | **Guided Policy Search**                                                                      | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf)</div>                                                     |
| ICML<br>2015<br> | **Trust Region Policy Optimization**                                                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1502.05477)</div>                                                                                 |
| 2017             | **Proximal Policy Optimization Algorithms**                                                   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1707.06347)</div>                                                                                 |
| 2017             | **Evolution Strategies as a Scalable Alternative to Reinforcement Learning**                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1703.03864)</div>                                                                                 |
|                  |                                                                                               |                                                                                                                                                                               |                                                                                                                                                                                                                             |


## Actor-Critic

| Date                | Title                                                                                                        | Summary                                                                                                                                                                                                                                                                                                          | Links                                                                                                                                                                                                    |
| ------------------- | ------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| NeurIPS<br>1999<br> | **Policy Gradient Methods for Reinforcement Learning with Function Approximation**                           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                    | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://papers.nips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)</div> |
| NeurIPS<br>1999     | **Actor-Critic Algorithms**                                                                                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                    | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://proceedings.neurips.cc/paper/1999/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html)</div> |
| ICML<br>2016<br>    | **Asynchronous Methods for Deep Reinforcement Learning**                                                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                    | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1602.01783)</div>                                                              |
| ICLR<br>2016<br>    | **High-Dimensional Continuous Control Using Generalized Advantage Estimation**                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                    | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1506.02438)</div>                                                              |
| ICLR<br>2017        | **Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic**                                       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                    | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1611.02247)</div>                                                              |
| ICML<br>2018        | **Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor** \| SAC | <sub>Provide an off-policy actor-critic algorithm, whose actor is not only consider maximizing the rewards, but also maximizing the entropy, in order to promote stochastic acting. Idea comes from **Soft Policy Iteration**, while the DeepRL method(SAC) is based on Q-Learning üåü \|üå∫\|üë©‚Äçüöí\|üëç\|ü§® </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1801.01290)</div>                                                              |
|                     |                                                                                                              |                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                          |



## Q-Learning

| Date               | Title                                                               | Summary                                                                                                                                                                       | Links                                                                                                                                                                           |
| ------------------ | ------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1989               | **Learning from Delayed Rewards**                                   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf)</div>                     |
| 2005               | **Neural Fitted Q Iteration**                                       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.endtoend.ai/assets/blog/paper-unraveled/nfq/original.pdf)</div> |
| IJCNN<br>2010<br>  | **Deep Auto-Encoder Neural Networks in Reinforcement Learning**     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5596468)</div>     |
| Nature<br>2013<br> | **Human-level control through deep reinforcement learning**         | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.nature.com/articles/nature14236)</div>                          |
| AAAI<br>2016<br>   | **Deep Reinforcement Learning with Double Q-learning**              | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1509.06461)</div>                                     |
| ICLR<br>2016       | **Continuous control with deep reinforcement learning** \| **DDPG** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1509.02971)</div>                                     |
| ICML<br>2016       | **Continuous Deep Q-Learning with Model-based Acceleration**        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1603.00748)</div>                                     |
| ICML<br>2016       | **Dueling Network Architectures for Deep Reinforcement Learning**   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1511.06581)</div>                                     |
|                    |                                                                     |                                                                                                                                                                               |                                                                                                                                                                                 |




## Planning

| Date            | Title                                                                                       | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                      |
| --------------- | ------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Trans<br>2012   | **A Survey of Monte Carlo Tree Search Methods**                                             | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://www.incompleteideas.net/609%20dropbox/other%20readings%20and%20resources/MCTS-survey.pdf)</div> |
| 1970            | **Differential dynamic programming**<br>                                                    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1099760)</div>                                    |
| IROS<br>2012    | **Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf)</div>                                 |
| NeurIPS<br>2014 | **Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics**       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://papers.nips.cc/paper_files/paper/2014/file/6766aa2750c19aad2fa1b32f36ed4aee-Paper.pdf)</div>   |
|                 |                                                                                             |                                                                                                                                                                               |                                                                                                                                                                                                            |



## Model-Based

| Date             | Title                                                                                               | Summary                                                                                                                                                                                                                                   | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ---------------- | --------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ICRA<br>2018     | **Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning** | <sub>Introduce a basic model-based RL with **Model Predictive Control(MPC)**. Gather data randomly first and MPC control for updating, which uses **random shooting** for finite-horizon. ‚ú®\|üå∏\|üë©‚Äçüöí\|üëçüèª\|ü§®</sub>                    | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1708.02596)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/mbmf)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/anagabandi/nn_dynamics)</div>                                              |
| ICML<br>2015     | **Weight Uncertainty in Neural Networks**                                                           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1505.05424)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICML<br>2011     | **PILCO: a model-based and data-efficient approach to policy search**                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf)</div>                                                                                                                                                                                                                                                                                                                                |
| NeurIPS<br>2018  | **Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models**          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1805.12114)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICML<br>2018     | **Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning**                    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1803.00101)</div>                                                                                                                                                                                                                                                                                                                                              |
| NeurIPS<br>2018  | **Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion**                | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1807.01675)</div>                                                                                                                                                                                                                                                                                                                                              |
| CoRL<br>2019     | **Deep Dynamics Models for Learning Dexterous Manipulation** \| PDDM                                | <sub>Provide a method of **online planning** with deep dynamics models. PDDM uses **a stronger optimizer** (better than **CEM** and **Random Shooting**) which considers covariances  and a softer update rule. ‚ú®\|üå∏\|‚òÑÔ∏è\|üëçüèª\|ü§®</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1909.11652)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/pddm/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/google-research/pddm)</div>                                               |
| NeurIPS <br>2015 | **Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images**            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1506.07365)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICML <br>2019    | **SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning**                   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1808.09105)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICRA <br>2017    | **Deep Visual Foresight for Planning Robot Motion**                                                 | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1610.00696)</div>                                                                                                                                                                                                                                                                                                                                              |
| CoRL <br>2017    | **Self-Supervised Visual Planning with Temporal Skip Connections**                                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1710.05268)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICML <br>2018    | **PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos**                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1902.01240)</div>                                                                                                                                                                                                                                                                                                                                              |
| 1991             | **Dyna, an integrated architecture for learning, planning, and reacting**                           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://dl.acm.org/doi/pdf/10.1145/122344.122377)</div>                                                                                                                                                                                                                                                                                                                              |
| ICML <br>2016    | **Continuous Deep Q-Learning with Model-based Acceleration**                                        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1603.00748)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICML <br>2018    | **Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning**                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1803.00101)</div>                                                                                                                                                                                                                                                                                                                                              |
| NeurIPS <br>2019 | **When to Trust Your Model: Model-Based Policy Optimization** \| MBPO                               | <sub>Use **SAC** to optimize the policy in MBPO method. Performing **short-term rollouts** from dynamics model to train policy is effective to reduce bias from model. ‚≠ê\|üå∏\|üë©‚Äçüöí\|üëçüèª\|ü§®</sub>                                       | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1906.08253)</div>                                                                                                                                                                                                                                                                                                                                              |
| 1993             | **Improving Generalisation for Temporal Difference Learning: The Successor Representation**         | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.gatsby.ucl.ac.uk/~dayan/papers/d93b.pdf)</div>                                                                                                                                                                                                                                                                                                                           |
| NeurIPS <br>2017 | **Successor Features for Transfer in Reinforcement Learning**                                       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1606.05312)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICLR <br>2021    | **C-Learning: Learning to Achieve Goals via Recursive Classification**                              | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2011.08909)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://ben-eysenbach.github.io/c_learning/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/google-research/google-research/tree/master/contrastive_rl)</div> |
|                  |                                                                                                     |                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |


## Exploration

| Date                     | Title                                                                                         | Summary                                                                                                                                                                                                                                                 | Links                                                                                                                                                                                                                                                                                                  |
| ------------------------ | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| NeurIPS<br>2016<br>      | **Unifying Count-Based Exploration and Intrinsic Motivation**                                 | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1606.01868)</div>                                                                                                                                                            |
| NeurIPS<br>2011          | **An Empirical Evaluation of Thompson Sampling**                                              | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://proceedings.neurips.cc/paper_files/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf)</div>                                                                                       |
| NeurIPS<br>2017          | **# Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning**         | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1611.04717)</div>                                                                                                                                                            |
| NeurIPS<br>2017          | **EX2: Exploration with Exemplar Models for Deep Reinforcement Learning**                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1703.01260)</div>                                                                                                                                                            |
| ICLR<br>2019             | **Exploration by Random Network Distillation** \| RND                                         | <sub>Introduce **a exploration bonus method**. Construct a fixed randomly initialized target network and a predictor network for training. Make the prediction error between the two networks as **an exploration bonus**. ‚≠ê\|üå∏\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|ü§®</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1810.12894)</div>                                                                                                                                                            |
| NeurIPS<br>2016          | **Deep Exploration via Bootstrapped DQN**                                                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1602.04621)</div>                                                                                                                                                            |
| NeurIPS<br>2016          | **VIME: Variational Information Maximizing Exploration**                                      | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1605.09674)</div>                                                                                                                                                            |
| 1992                     | **A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://sferics.idsia.ch/pub/juergen/curiositysab.pdf)</div>                                                                                                                                       |
| ICLR<br>2016             | **Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models**           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1507.00814)</div>                                                                                                                                                            |
| NeurIPS<br>2018          | **Visual Reinforcement Learning with Imagined Goals**                                         | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1807.04742)</div>                                                                                                                                                            |
| ICML<br>2020             | **Skew-Fit: State-Covering Self-Supervised Reinforcement Learning**                           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1903.03698)</div>                                                                                                                                                            |
| Jun/2019                 | **Efficient Exploration via State Marginal Matching**                                         | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1906.05274)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/state-marginal-matching)</div> |
| ICML<br>2019             | **Provably Efficient Maximum Entropy Exploration**                                            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1812.02690)</div>                                                                                                                                                            |
| Jun/2018                 | **Unsupervised Meta-Learning for Reinforcement Learning**                                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1806.04640)</div>                                                                                                                                                            |
| ICLR<br>2019             | **Diversity is All You Need: Learning Skills without a Reward Function**                      | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1802.06070)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/diayn/)</div>                  |
| ICLR<br>workshop<br>2017 | **Variational Intrinsic Control**                                                             | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1611.07507)</div>                                                                                                                                                            |
| ICML<br>1998             | **Finite-time Analysis of the Multiarmed Bandit Problem**                                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf)</div>                                                                                                                                 |
| NeuIPS<br>2014           | **Learning to Optimize via Information-Directed Sampling**                                    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1403.5556)</div>                                                                                                                                                             |
| 2010                     | **Formal Theory of Creativity, Fun, and Intrinsic Motivation**                                | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://people.idsia.ch/~juergen/ieeecreative.pdf)</div>                                                                                                                                           |
| ICLR<br>2021             | **Parrot: Data-Driven Behavioral Priors for Reinforcement Learning**                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2011.10024)</div>                                                                                                                                                            |
|                          |                                                                                               |                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                        |


## Offline RL

| Date            | Title                                                                                    | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Links                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| --------------- | ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2020            | **Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems**  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2005.01643)</div>                                                                                                                                                                                                                                                                                            |
| CoRL<br>2020    | **COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning**    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2010.14500)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/cog-rl)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/avisingh599/cog)</div> |
| CoRL<br>2018    | **QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation**   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1806.10293)</div>                                                                                                                                                                                                                                                                                            |
| NeurIPS<br>2019 | **Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction**                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1906.00949)</div>                                                                                                                                                                                                                                                                                            |
| ICML<br>2016    | **Doubly Robust Off-policy Value Evaluation for Reinforcement Learning**                 | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1511.03722)</div>                                                                                                                                                                                                                                                                                            |
| 2019            | **Behavior Regularized Offline Reinforcement Learning**                                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1911.11361)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/google-research/google-research/tree/master/behavior_regularized_offline_rl)</div>                                                                               |
| 2019            | **Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1910.00177)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://xbpeng.github.io/projects/AWR/)</div>                                                                                                                                                |
| 2020            | **AWAC: Accelerating Online Reinforcement Learning with Offline Datasets** \| AWAC       | <sub>Improve the actor update based on the AC framework, to avoid offline bootstrap error as well as overly conservative updates. Instead of using KL divergence explicitly, it transforms the policy constraint into **an implicit form** with Lagrangian. üí´\|üå∑\|‚òÑÔ∏è\|üëçüèΩ\|ü§® </sub>                                                                                                                                                                                                                                                                                                                                            | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2006.09359)</div>                                                                                                                                                                                                                                                                                            |
| ICLR<br>2022    | **Offline Reinforcement Learning with Implicit Q-Learning** \| IQL                       | <sub>Provide a **"on-policy"** type offline method, to avoid querying out-of-sample (unseen) actions in TD loss. Specifically, it introduces a separate value function which is trained through **expectile regression** **only** utilizing **the action from the dataset**. Then it updates the Q-functions with this value. Beyond the above, it then extracts a separated policy using advantage weighted regression like **AWAC** algorithm. ‚≠ê\|üå∏\|‚ù§Ô∏è‚Äçüî•\|üëç \|ü§®</sub>                                                                                                                                                       | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2110.06169)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/ikostrikov/implicit_q_learning)</div>                                                                                                                            |
| NeurIPS<br>2020 | **Conservative Q-Learning for Offline Reinforcement Learning** \| CQL                    | <sub>Provide a conservative low-bound Q-function to prevent the over-estimation that is common in offline RL settings due to OOD actions and function approximation error. Alongside the standard Bellman error objective, it adds **a penalty to minimize the expected Q-value(Average term)** and **an additional Q-value maximization term(Chosen term)** to substantially tighten this bound. ‚≠ê\|üå∏\|‚òÑÔ∏è\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                        | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2006.04779)</div>                                                                                                                                                                                                                                                                                            |
| NeurIPS<br>2020 | **MOPO: Model-based Offline Policy Optimization**                                        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2005.13239)</div>                                                                                                                                                                                                                                                                                            |
| NeurIPS<br>2020 | **MOReL : Model-Based Offline Reinforcement Learning**                                   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2005.05951)</div>                                                                                                                                                                                                                                                                                            |
| NeurIPS<br>2021 | **COMBO: Conservative Offline Model-Based Policy Optimization**                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2102.08363)</div>                                                                                                                                                                                                                                                                                            |
| NeurIPS<br>2021 | **Offline Reinforcement Learning as One Big Sequence Modeling Problem**                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2106.02039)</div>                                                                                                                                                                                                                                                                                            |
| ICML<br>2021    | **Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills**     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2104.07749)</div>                                                                                                                                                                                                                                                                                            |
| CoRL<br>2021    | **A Workflow for Offline Model-Free Robotic Reinforcement Learning**                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2109.10813)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/offline-rl-workflow)</div>                                                                                                                                     |
| ICML<br>2024    | **DIDI: Diffusion-Guided Diversity for Offline Behavioral Generation**                   | Propose a novel approach called **DIDI** for offline behavioral generation, aiming to learn diverse behaviors from a mixture of label-free offline data. To control the behavioral generation, DIDI introduce **a contextual policy** that can be commanded to produce specific behaviors. Then DIDI use **a diffusion probabilistic model** as a prior to guide the learning of the contextual policy. In a word, **DIDI** learns **a offline latent-conditioned policy** incorporating a variable(**pre-trained diffusion prior**) in offline regularization, to balances diversity and training stability. üí´\|üíê\|üî•\|üëçüèΩ\|ü§® | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2405.14790)</div>                                                                                                                                                                                                                                                                                            |
|                 |                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                        |



## Importance Sampling

| Date         | Title                                                                      | Summary                                                                                                                                                                       | Links                                                                                                                                                                                              |
| ------------ | -------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2000         | **Eligibility Traces for Off-Policy Policy Evaluation**                    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1079&context=cs_faculty_pubs)</div> |
| 2002         | **Learning from Scarce Experience**                                        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/cs/0204043)</div>                                                        |
| ICML<br>2016 | **Doubly Robust Off-policy Value Evaluation for Reinforcement Learning**   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1511.03722)</div>                                                        |
| ICML<br>2016 | **Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1604.00923)</div>                                                        |
| AAAI<br>2015 | **High Confidence Off-Policy Evaluation**                                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://people.cs.umass.edu/~pthomas/papers/Thomas2015.pdf)</div>                              |
| 2017         | **Consistent On-Line Off-Policy Evaluation**                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1702.07121)</div>                                                        |
| UAI<br>2019  | **Off-Policy Policy Gradient with State Distribution Correction**          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1904.08473)</div>                                                        |
|              |                                                                            |                                                                                                                                                                               |                                                                                                                                                                                                    |



## Inference

| Date            | Title                                                                                                 | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                    |
| --------------- | ----------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| NeurIPS<br>2006 | **Linearly-solvable Markov decision problems**                                                        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://papers.nips.cc/paper_files/paper/2006/file/d806ca13ca3449af72a1ea5aedbed26a-Paper.pdf)</div> |
| CDC<br>2008     | **General duality between optimal control and estimation**                                            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://homes.cs.washington.edu/~todorov/papers/TodorovCDC08.pdf)</div>                              |
| 2009            | **Optimal control as a graphical model inference problem**                                            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/0901.0633)</div>                                                               |
| ICML<br>2010    | **Modeling Interaction via the Principle of Maximum Causal Entropy**                                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.cs.cmu.edu/~bziebart/publications/maximum-causal-entropy.pdf)</div>                      |
| RSS<br>2012     | **On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference**                 | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.roboticsproceedings.org/rss08/p45.pdf)</div>                                             |
| ICML<br>2017    | **Reinforcement Learning with Deep Energy-Based Policies**                                            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1702.08165)</div>                                                              |
| NeurIPS<br>2017 | **Bridging the Gap Between Value and Policy Based Reinforcement Learning**                            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1702.08892)</div>                                                              |
| 2017            | **Equivalence Between Policy Gradients and Soft Q-Learning**                                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1704.06440)</div>                                                              |
| ICML<br>2018    | **Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1801.01290)</div>                                                              |
| 2018            | **Soft Actor-Critic Algorithms and Applications**                                                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1812.05905)</div>                                                              |
| 2018            | **Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review**                | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1805.00909)</div>                                                              |
| ICRA<br>2018    | **Composable Deep Reinforcement Learning for Robotic Manipulation**                                   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1803.06773)</div>                                                              |
| RSS<br>2019     | **Learning to Walk via Deep Reinforcement Learning**                                                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1812.11103)</div>                                                              |
|                 |                                                                                                       |                                                                                                                                                                               |                                                                                                                                                                                                          |


## Variational Inference

| Date            | Title                                                                                        | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                  |
| --------------- | -------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| CoRL<br>2019    | **Learning Latent Plans from Play**                                                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1903.01973)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://learning-from-play.github.io/)</div> |
| RSS<br>2023     | **Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware**                       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2304.13705)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://tonyzhaozh.github.io/aloha/)</div>   |
| NeurIPS<br>2015 | **Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images**     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1506.07365)</div>                                                                                                                                            |
| ICML<br>2019    | **SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning**            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1808.09105)</div>                                                                                                                                            |
| ICML<br>2019    | **Learning Latent Dynamics for Planning from Pixels**                                        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1811.04551)</div>                                                                                                                                            |
| NeurIPS<br>2020 | **Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1907.00953)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://alexlee-gk.github.io/slac/)</div>    |
| ICLR<br>2020    | **Dream to Control: Learning Behaviors by Latent Imagination**                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1912.01603)</div>                                                                                                                                            |
|                 |                                                                                              |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                        |



## Inverse RL

| Date            | Title                                                                                                             | Summary                                                                                                                                                                       | Links                                                                                                                                                                |
| --------------- | ----------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ICML<br>2004    | **Apprenticeship Learning via Inverse Reinforcement Learning**                                                    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf)</div> |
| AAAI<br>2008    | **Maximum Entropy Inverse Reinforcement Learning**                                                                | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf)</div>             |
| ICML<br>2016    | **Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization**                                    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/1603.00448.pdf)</div>                      |
| 2016            | **Maximum Entropy Deep Inverse Reinforcement Learning**                                                           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1507.04888)</div>                          |
| NeurIPS<br>2016 | **Generative Adversarial Imitation Learning**                                                                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1606.03476)</div>                          |
| ICLR<br>2018    | **Learning Robust Rewards with Adversarial Inverse Reinforcement Learning**                                       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1710.11248)</div>                          |
| 2016            | **A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1611.03852)</div>                          |
|                 |                                                                                                                   |                                                                                                                                                                               |                                                                                                                                                                      |


## Sequence Model

| Date            | Title                                                                                 | Summary                                                                                                                                                                      | Links                                                                                                                                                 |
| --------------- | ------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| ICLR<br>2019    | **Recurrent Experience Replay in Distributed Reinforcement Learning**                 | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://openreview.net/forum?id=r1lyTjAqYX)</div> |
| 2019            | **Fine-Tuning Language Models from Human Preferences**                                | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1909.08593)</div>           |
| 2022            | **Training language models to follow instructions with human feedback**               | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2203.02155)</div>           |
| ICCV<br>2019    | **Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning**        | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1703.06585)</div>           |
| EMNLP<br>2020   | **Human-centric Dialog Training via Offline Reinforcement Learning**                  | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2010.05848)</div>           |
| ACL<br>2022     | **CHAI: A CHatbot AI for Task-Oriented Dialogue with Offline Reinforcement Learning** | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1909.08593)</div>           |
| ICLR<br>2023    | **Offline RL for Natural Language Generation with Implicit Language Q Learning**      | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2206.11871)</div>           |
| NeurIPS<br>2017 | **Deep reinforcement learning from human preferences**                                | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1706.03741)</div>           |
| 2023            | **Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations**                 | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2311.05584)</div>           |
|                 |                                                                                       |                                                                                                                                                                              |                                                                                                                                                       |


## Transfer Learning

| Date            | Title                                                                                            | Summary                                                                                                                                                                       | Links                                                                                                                                       |
| --------------- | ------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| ICLR<br>2021    | **Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers**           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2006.13916)</div> |
| ICLR<br>2017    | **EPOpt: Learning Robust Neural Network Policies Using Model Ensembles**                         | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1610.01283)</div> |
| 2016            | **CAD2RL: Real Single-Image Flight without a Single Real Image**                                 | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1610.01283)</div> |
| ICRA<br>2018    | **Sim-to-Real Transfer of Robotic Control with Dynamics Randomization**                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1710.06537)</div> |
| Nature<br>2020  | **Learning Quadrupedal Locomotion over Challenging Terrain**                                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2010.11251)</div> |
| 2014            | **Deep Domain Confusion: Maximizing for Domain Invariance**                                      | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1412.3474)</div>  |
| 2015            | **Domain-Adversarial Training of Neural Networks**                                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1505.07818)</div> |
| 2015            | **Adapting Deep Visuomotor Representations with Weak Pairwise Constraints**                      | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1511.07111)</div> |
| ICML<br>2017    | **Reinforcement Learning with Deep Energy-Based Policies**                                       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1702.08165)</div> |
| ICML<br>2017    | **Modular Multitask Reinforcement Learning with Policy Sketches**                                | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1611.01796)</div> |
| ICLR<br>2017    | Stochastic Neural Networks for Hierarchical Reinforcement Learning                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1704.03012)</div> |
| NeurIPS<br>2020 | **One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL**            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2010.14484)</div> |
| 2017            | **Preparing for the Unknown: Learning a Universal Policy with Online System Identification**     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1702.02453)</div> |
| IROS<br>2017    | **Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1703.06907)</div> |
| RSS<br>2018     | **Sim-to-Real: Learning Agile Locomotion For Quadruped Robots**                                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1804.10332)</div> |
|                 |                                                                                                  |                                                                                                                                                                               |                                                                                                                                             |

## Multi-Task Learning

| Date            | Title                                                                                                                               | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                              |
| --------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 1993            | **Learning to Achieve Goals**                                                                                                       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=6df43f70f383007a946448122b75918e3a9d6682)</div> |
| ICML<br>2015    | **Universal Value Function Approximators**                                                                                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://proceedings.mlr.press/v37/schaul15.pdf)</div>                                                          |
| NeurIPS<br>2017 | **Hindsight Experience Replay**                                                                                                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1707.01495)</div>                                                                        |
| ICLR<br>2021    | **C-Learning: Learning to Achieve Goals via Recursive Classification**                                                              | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2011.08909)</div>                                                                        |
| ICRA<br>2021    | **Reset-Free Reinforcement Learning via Multi-Task Learning: Learning Dexterous Manipulation Behaviors without Human Intervention** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2104.11203)</div>                                                                        |
|                 |                                                                                                                                     |                                                                                                                                                                               |                                                                                                                                                                                                                    |


## Meta-Learning

| Date            | Title                                                                                    | Summary                                                                                                                                                                       | Links                                                                                                                                                                                    |
| --------------- | ---------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| NeurIPS<br>2015 | **Memory-based control with recurrent neural networks**                                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1512.04455)</div>                                              |
| 2016            | **Learning to reinforcement learn**                                                      | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1611.05763)</div>                                              |
| ICLR<br>2017    | **RL2: Fast Reinforcement Learning via Slow Reinforcement Learning**                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1611.02779)</div>                                              |
| ICLR<br>2018    | **A Simple Neural Attentive Meta-Learner**                                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1707.03141)</div>                                              |
| ICML<br>2019    | **Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1903.08254)</div>                                              |
| ICML<br>2017    | **Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks**                    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1703.03400)</div>                                              |
| ICML<br>2018    | **DiCE: The Infinitely Differentiable Monte-Carlo Estimator**                            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1802.05098)</div>                                              |
| ICLR<br>2019    | **ProMP: Proximal Meta-Policy Search**                                                   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1810.06784)</div>                                              |
| NeurIPS<br>2018 | **Meta-Reinforcement Learning of Structured Exploration Strategies**                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1802.07245)</div>                                              |
| NeurIPS<br>2018 | **Some Considerations on Learning to Explore via Meta-Reinforcement Learning**           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1803.01118)</div>                                              |
| NeurIPS<br>2018 | **Evolved Policy Gradients**                                                             | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1802.04821)</div>                                              |
| GECCO<br>2018   | **Meta-Learning by the Baldwin Effect**                                                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1806.07917)</div>                                              |
| 2013            | **Learning to Optimize Via Posterior Sampling**                                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1301.2609)</div>                                               |
| ICML<br>2019    | **Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1903.08254)</div>                                              |
| ICLR<br>2019    | **Variational Task Embeddings For Fast Adaptation In Deep Reinforcement Learning**       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://eringrant.github.io/spirl/2019/camera-ready/spirl_camera-ready_30.pdf)</div> |
| ICLR<br>2020    | **Meta reinforcement learning as task inference**                                        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1905.06424)</div>                                              |
| ICML<br>2018    | **Been There, Done That: Meta-Learning with Episodic Recall**                            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1805.09692)</div>                                              |
| Nature<br>2018  | **Prefrontal cortex as a meta-reinforcement learning system**                            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.nature.com/articles/s41593-018-0147-8)</div>                             |
| 2019            | **Causal Reasoning from Meta-reinforcement Learning**                                    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1901.08162)</div>                                              |
|                 |                                                                                          |                                                                                                                                                                               |                                                                                                                                                                                          |




## Books

* [Reinforcement Learning : Theory and Algorithms](https://rltheorybook.github.io/)


## Researchers

| Date             | Title                                                                                                          | Summary                                                                                                                                                                       | Links                                                                                                                                 |
| ---------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| 7/4/23<br>ICML23 | **Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![Note](https://img.shields.io/badge/Note-Read-blue?logo=dependabot)](summary/2024-03/2403.18349.md)</div> |
|                  |                                                                                                                |                                                                                                                                                                               |                                                                                                                                       |
