#  Reinforcement Learning



* [Imitation Learning](#imitation-learning)
* [Policy Gradient](#policy-gradient)
* [Actor-Critic](#actor-critic)
* [Q-Learning](#q-learning)
* [Planning](#planning)
* [Model-Based](#model-based)
* [Exploration](#exploration)
* [Offline RL](#offline-rl)
	*  [Importance Sampling](#importance-sampling)
* [Standard](#standard)




##  Imitation Learning

| Date            | Title                                                                                                        | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                          |
| --------------- | ------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| AISTATS<br>2011 | **A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning**                 | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1011.0686)</div>                                                                                                                                                     |
| ICRA<br>2018    | **Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-To-End Learning from Demonstration** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1707.02920)</div>                                                                                                                                                    |
| NeurIPS<br>2019 | **Causal Confusion in Imitation Learning**                                                                   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1905.11979)</div>                                                                                                                                                    |
| RSS<br>2023     | **Diffusion Policy: Visuomotor Policy Learning via Action Diffusion**                                        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2303.04137)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://diffusion-policy.cs.columbia.edu/)</div>     |
| RSS<br>2023     | **Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware**                                       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2304.13705)</div>                                                                                                                                                    |
| 2022            | **RT-1: Robotics Transformer for Real-World Control at Scale**                                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2212.06817)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robotics-transformer1.github.io/)</div>      |
| CoRL<br>2019    | **Learning Latent Plans from Play**                                                                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1903.01973)</div>                                                                                                                                                    |
| RSS<br>2019     | **Unsupervised Visuomotor Control through Distributional Planning Networks**                                 | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1902.05542)</div>                                                                                                                                                    |
| ICRA<br>2023    | **GNM: A General Navigation Model to Drive Any Robot**                                                       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2210.03370)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/drive-any-robot)</div> |
|                 |                                                                                                              |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                |



## Policy Gradient

| Date             | Title                                                                                         | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                       |
| ---------------- | --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1992             | **Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://link.springer.com/content/pdf/10.1023/A:1022672621406.pdf)</div>                                                |
| 2001             | **Infinite-Horizon Policy-Gradient Estimation**                                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1106.0665)</div>                                                                                  |
| 2008             | **Reinforcement learning of motor skills with policy gradients**                              | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Publications/Neural-Netw-2008-21-682_4867[0].pdf)</div> |
| ICML<br>2013<br> | **Guided Policy Search**                                                                      | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf)</div>                                                     |
| ICML<br>2015<br> | **Trust Region Policy Optimization**                                                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1502.05477)</div>                                                                                 |
| 2017             | **Proximal Policy Optimization Algorithms**                                                   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)]()</div>                                                                                                                 |
|                  |                                                                                               |                                                                                                                                                                               |                                                                                                                                                                                                                             |



## Actor-Critic

| Date                | Title                                                                                                 | Summary                                                                                                                                                                                                                                    | Links                                                                                                                                                                                                    |
| ------------------- | ----------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| NeurIPS<br>1999<br> | **Policy Gradient Methods for Reinforcement Learning with Function Approximation**                    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                              | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://papers.nips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)</div> |
| NeurIPS<br>1999     | **Actor-Critic Algorithms**                                                                           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                              | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://proceedings.neurips.cc/paper/1999/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html)</div> |
| ICML<br>2016<br>    | **Asynchronous Methods for Deep Reinforcement Learning**                                              | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                              | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1602.01783)</div>                                                              |
| ICLR<br>2016<br>    | **High-Dimensional Continuous Control Using Generalized Advantage Estimation**                        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                              | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1506.02438)</div>                                                              |
| ICLR<br>2017        | **Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic**                                | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                              | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1611.02247)</div>                                                              |
| ICML<br>2018        | **Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor** | <sub>Provide an off-policy actor-critic algorithm, whose actor is not only consider maximizing the rewards, but also maximizing the entropy, in order to promote stochastic acting. **Soft Policy Iteration**. ⭐\|🌺\|👩‍🚒\|👍\|🤨 </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1801.01290)</div>                                                              |
|                     |                                                                                                       |                                                                                                                                                                                                                                            |                                                                                                                                                                                                          |



## Q-Learning

| Date               | Title                                                             | Summary                                                                                                                                                                       | Links                                                                                                                                                                           |
| ------------------ | ----------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1989               | **Learning from Delayed Rewards**                                 | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf)</div>                     |
| 2005               | **Neural Fitted Q Iteration**                                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.endtoend.ai/assets/blog/paper-unraveled/nfq/original.pdf)</div> |
| IJCNN<br>2010<br>  | **Deep Auto-Encoder Neural Networks in Reinforcement Learning**   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5596468)</div>     |
| Nature<br>2013<br> | **Human-level control through deep reinforcement learning**       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.nature.com/articles/nature14236)</div>                          |
| AAAI<br>2016<br>   | **Deep Reinforcement Learning with Double Q-learning**            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1509.06461)</div>                                     |
| ICLR<br>2016       | **Continuous control with deep reinforcement learning**           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1509.02971)</div>                                     |
| ICML<br>2016       | **Continuous Deep Q-Learning with Model-based Acceleration**      | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1603.00748)</div>                                     |
| ICML<br>2016       | **Dueling Network Architectures for Deep Reinforcement Learning** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1511.06581)</div>                                     |
|                    |                                                                   |                                                                                                                                                                               |                                                                                                                                                                                 |




## Planning

| Date            | Title                                                                                       | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                      |
| --------------- | ------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Trans<br>2012   | **A Survey of Monte Carlo Tree Search Methods**                                             | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://www.incompleteideas.net/609%20dropbox/other%20readings%20and%20resources/MCTS-survey.pdf)</div> |
| 1970            | **Differential dynamic programming**<br>                                                    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1099760)</div>                                    |
| IROS<br>2012    | **Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf)</div>                                 |
| NeurIPS<br>2014 | **Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics**       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://papers.nips.cc/paper_files/paper/2014/file/6766aa2750c19aad2fa1b32f36ed4aee-Paper.pdf)</div>   |
|                 |                                                                                             |                                                                                                                                                                               |                                                                                                                                                                                                            |




## Model-Based

| Date             | Title                                                                                               | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ---------------- | --------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ICRA<br>2018     | **Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1708.02596)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/mbmf)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/anagabandi/nn_dynamics)</div>                                              |
| ICML<br>2015     | **Weight Uncertainty in Neural Networks**                                                           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1505.05424)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICML<br>2011     | **PILCO: a model-based and data-efficient approach to policy search**                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf)</div>                                                                                                                                                                                                                                                                                                                                |
| NeurIPS<br>2018  | **Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models**          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1805.12114)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICML<br>2018     | **Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning**                    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1803.00101)</div>                                                                                                                                                                                                                                                                                                                                              |
| NeurIPS<br>2018  | **Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion**                | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1807.01675)</div>                                                                                                                                                                                                                                                                                                                                              |
| CoRL<br>2019     | **Deep Dynamics Models for Learning Dexterous Manipulation**                                        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1909.11652)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/pddm/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/google-research/pddm)</div>                                               |
| NeurIPS <br>2015 | **Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images**            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1506.07365)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICML <br>2019    | **SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning**                   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1808.09105)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICRA <br>2017    | **Deep Visual Foresight for Planning Robot Motion**                                                 | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1610.00696)</div>                                                                                                                                                                                                                                                                                                                                              |
| CoRL <br>2017    | **Self-Supervised Visual Planning with Temporal Skip Connections**                                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1710.05268)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICML <br>2018    | **PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos**                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1902.01240)</div>                                                                                                                                                                                                                                                                                                                                              |
| 1991             | **Dyna, an integrated architecture for learning, planning, and reacting**                           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://dl.acm.org/doi/pdf/10.1145/122344.122377)</div>                                                                                                                                                                                                                                                                                                                              |
| ICML <br>2016    | **Continuous Deep Q-Learning with Model-based Acceleration**                                        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1603.00748)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICML <br>2018    | **Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning**                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1803.00101)</div>                                                                                                                                                                                                                                                                                                                                              |
| NeurIPS <br>2019 | **When to Trust Your Model: Model-Based Policy Optimization**                                       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1906.08253)</div>                                                                                                                                                                                                                                                                                                                                              |
| 1993             | **Improving Generalisation for Temporal Difference Learning: The Successor Representation**         | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.gatsby.ucl.ac.uk/~dayan/papers/d93b.pdf)</div>                                                                                                                                                                                                                                                                                                                           |
| NeurIPS <br>2017 | **Successor Features for Transfer in Reinforcement Learning**                                       | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1606.05312)</div>                                                                                                                                                                                                                                                                                                                                              |
| ICLR <br>2021    | **C-Learning: Learning to Achieve Goals via Recursive Classification**                              | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2011.08909)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://ben-eysenbach.github.io/c_learning/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/google-research/google-research/tree/master/contrastive_rl)</div> |
|                  |                                                                                                     |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |


## Exploration

| Date                     | Title                                                                                         | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                                  |
| ------------------------ | --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| NeurIPS<br>2016<br>      | **Unifying Count-Based Exploration and Intrinsic Motivation**                                 | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1606.01868)</div>                                                                                                                                                            |
| NeurIPS<br>2011          | **An Empirical Evaluation of Thompson Sampling**                                              | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://proceedings.neurips.cc/paper_files/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf)</div>                                                                                       |
| NeurIPS<br>2017          | **# Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning**         | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1611.04717)</div>                                                                                                                                                            |
| NeurIPS<br>2017          | **EX2: Exploration with Exemplar Models for Deep Reinforcement Learning**                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1703.01260)</div>                                                                                                                                                            |
| ICLR<br>2019             | **Exploration by Random Network Distillation**                                                | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1810.12894)</div>                                                                                                                                                            |
| NeurIPS<br>2016          | **Deep Exploration via Bootstrapped DQN**                                                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1602.04621)</div>                                                                                                                                                            |
| NeurIPS<br>2016          | **VIME: Variational Information Maximizing Exploration**                                      | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1605.09674)</div>                                                                                                                                                            |
| 1992                     | **A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://sferics.idsia.ch/pub/juergen/curiositysab.pdf)</div>                                                                                                                                       |
| ICLR<br>2016             | **Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models**           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1507.00814)</div>                                                                                                                                                            |
| NeurIPS<br>2018          | **Visual Reinforcement Learning with Imagined Goals**                                         | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1807.04742)</div>                                                                                                                                                            |
| ICML<br>2020             | **Skew-Fit: State-Covering Self-Supervised Reinforcement Learning**                           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1903.03698)</div>                                                                                                                                                            |
| Jun/2019                 | **Efficient Exploration via State Marginal Matching**                                         | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1906.05274)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/state-marginal-matching)</div> |
| ICML<br>2019             | **Provably Efficient Maximum Entropy Exploration**                                            | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1812.02690)</div>                                                                                                                                                            |
| Jun/2018                 | **Unsupervised Meta-Learning for Reinforcement Learning**                                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1806.04640)</div>                                                                                                                                                            |
| ICLR<br>2019             | **Diversity is All You Need: Learning Skills without a Reward Function**                      | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1802.06070)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/diayn/)</div>                  |
| ICLR<br>workshop<br>2017 | **Variational Intrinsic Control**                                                             | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1611.07507)</div>                                                                                                                                                            |
|                          |                                                                                               |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                        |


## Offline RL

| Date            | Title                                                                                    | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| --------------- | ---------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2020            | **Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems**  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2005.01643)</div>                                                                                                                                                                                                                                                                                            |
| CoRL<br>2020    | **COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning**    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2010.14500)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/cog-rl)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/avisingh599/cog)</div> |
| CoRL<br>2018    | **QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation**   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1806.10293)</div>                                                                                                                                                                                                                                                                                            |
| NeurIPS<br>2019 | **Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction**                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1906.00949)</div>                                                                                                                                                                                                                                                                                            |
| ICML<br>2016    | **Doubly Robust Off-policy Value Evaluation for Reinforcement Learning**                 | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1511.03722)</div>                                                                                                                                                                                                                                                                                            |
| 2019            | **Behavior Regularized Offline Reinforcement Learning**                                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1911.11361)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/google-research/google-research/tree/master/behavior_regularized_offline_rl)</div>                                                                               |
| 2019            | **Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1910.00177)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://xbpeng.github.io/projects/AWR/)</div>                                                                                                                                                |
| 2020            | **AWAC: Accelerating Online Reinforcement Learning with Offline Datasets**               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2006.09359)</div>                                                                                                                                                                                                                                                                                            |
| ICLR<br>2022    | **Offline Reinforcement Learning with Implicit Q-Learning**                              | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2110.06169)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/ikostrikov/implicit_q_learning)</div>                                                                                                                            |
| NeurIPS<br>2020 | **Conservative Q-Learning for Offline Reinforcement Learning**                           | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2006.04779)</div>                                                                                                                                                                                                                                                                                            |
| NeurIPS<br>2020 | **MOPO: Model-based Offline Policy Optimization**                                        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2005.13239)</div>                                                                                                                                                                                                                                                                                            |
| NeurIPS<br>2020 | **MOReL : Model-Based Offline Reinforcement Learning**                                   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2005.05951)</div>                                                                                                                                                                                                                                                                                            |
| NeurIPS<br>2021 | **COMBO: Conservative Offline Model-Based Policy Optimization**                          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2102.08363)</div>                                                                                                                                                                                                                                                                                            |
| NeurIPS<br>2021 | **Offline Reinforcement Learning as One Big Sequence Modeling Problem**                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2106.02039)</div>                                                                                                                                                                                                                                                                                            |
| ICML<br>2021    | **Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills**     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2104.07749)</div>                                                                                                                                                                                                                                                                                            |
| CoRL<br>2021    | **A Workflow for Offline Model-Free Robotic Reinforcement Learning**                     | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2109.10813)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/offline-rl-workflow)</div>                                                                                                                                     |
|                 |                                                                                          |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                        |


### Importance Sampling

| Date         | Title                                                                      | Summary                                                                                                                                                                       | Links                                                                                                                                                                                              |
| ------------ | -------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2000         | **Eligibility Traces for Off-Policy Policy Evaluation**                    | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1079&context=cs_faculty_pubs)</div> |
| 2002         | **Learning from Scarce Experience**                                        | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/cs/0204043)</div>                                                        |
| ICML<br>2016 | **Doubly Robust Off-policy Value Evaluation for Reinforcement Learning**   | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1511.03722)</div>                                                        |
| ICML<br>2016 | **Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1604.00923)</div>                                                        |
| AAAI<br>2015 | **High Confidence Off-Policy Evaluation**                                  | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://people.cs.umass.edu/~pthomas/papers/Thomas2015.pdf)</div>                              |
| 2017         | **Consistent On-Line Off-Policy Evaluation**                               | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1702.07121)</div>                                                        |
| UAI<br>2019  | **Off-Policy Policy Gradient with State Distribution Correction**          | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/1904.08473)</div>                                                        |
|              |                                                                            |                                                                                                                                                                               |                                                                                                                                                                                                    |




## Standard

| Date             | Title                                                                                                          | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ---------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 7/4/23<br>ICML23 | **Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openai/weak-to-strong)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Posts-yellow?logo=rss)](https://mp.weixin.qq.com/s/f6YW-CxnLhnfMWTLg4M4Cw)</div><div style='width:150px;'>[![Note](https://img.shields.io/badge/Note-Read-blue?logo=dependabot)](summary/2024-03/2403.18349.md)</div> |
|                  |                                                                                                                |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
