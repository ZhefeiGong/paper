# Large Language Model


**üìãCatalogue**
* [Review](#review)

**üî¨Resources**
* [KnowledgeBase](./llm_knowledge_base.md)
* [Standard](#standard)


## Benchmark

| Date            | Title                                                                                        | Summary                                                                                                                                                                                                                                                                                                            | Links                                                                                                                                                                                                                                                                                                                                                                                                                              |
| --------------- | -------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| NeurIPS<br>2022 | **Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering** | <sub>Provide **ScienceQA** benchmark, a new benchmark that consists of **‚àº21k** multimodal multiple choice questions with diverse science topics and annotations of their answers with corresponding lectures and explanations, for learning the **chain of thought (CoT)** of LLM. üí´\|üå∑\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|üòâ </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2209.09513)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://scienceqa.github.io/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/lupantech/ScienceQA)</div> |
|                 |                                                                                              |                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## Large Language Model / LLM
| Date | Title                                                    | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Links                                                                                                                                                                                                                                                                                         |
| ---- | -------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2023 | **LLaMA: Open and Efficient Foundation Language Models** | <sub>Proposed **LLaMA 1**, an open-source LLM(GPT framework). **LLaMA** is a network based on the transformer architecture with the three main difference. (1)Pre-normalization, using **RMSNorm** to normalize the input of each transformer sub-layer. (2)**SwiGLU** activation function, replacing the ReLU non-linearity. (3)**Rotary** Embeddings, removing the absolute positional embeddings and instead using **RoPE**. For efficient implementation, **LLaMA** uses an efficient implementation of **the causal multi-head attention**, and save activations which are recomputed during the backward pass through **checkpointing**. ‚≠ê\|üå∏\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|üòâ</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2302.13971)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/meta-llama/llama)</div> |
| 2023 | **Llama 2: Open Foundation and Fine-Tuned Chat Models**  | <sub>Proposed **LLaMA 2**. The primary architectural differences from **Llama 1** include **increased context length** and **grouped-query attention (GQA)**. And also proposed **LLaMA 2 chatbot** that is fine-tuned with **RLHF**. üí´\|üå∏\|‚ù§Ô∏è‚Äçüî•\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                         | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2307.09288)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/meta-llama/llama)</div> |
| 2024 | **The Llama 3 Herd of Models**                           | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2302.13971)</div>                                                                                                                                                   |
|      |                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                               |
|      |                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                               |


## Vision-Language Model / VLM

| Date            | Title                                                 | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Links                                                                                                                                                                                                                                                                        |
| --------------- | ----------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2023<br>NeurIPS | **Visual Instruction Tuning**                         | <sub>Propose **LLaVA** (Large Language and Vision Assistant), an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general purpose visual and language understanding. It's a **VLM** built upon LLaMA, Vicuna, and CLIP. In a nutshell, LLaVA uses a pre-trained CLIP visual encoder to process the input image into **image features**. Then it uses **a trainable projection matrix**  to convert the features into language embedding tokens. Thus, for each image, LLaVA generates multi-turn conversation data to instruction-tuning the LLM. ‚≠ê\|üå∏\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|ü§®</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2304.08485)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://llava-vl.github.io/)</div> |
| 2024<br>CVPR    | **Improved Baselines with Visual Instruction Tuning** | <sub>Proposed **LLaVA-1.5**. With simple modifications to LLaVA, namely, using **CLIP-ViT-L-336px with an MLP projection** and **adding academic-task-oriented VQA(Visual Question Answering) data with response formatting prompts**, LLaVA-1.5 established stronger baselines that achieve state-of-the-art across 11 benchmarks. ‚ú®\|üíê\|üî•\|üëçüèø\|üòâ</sub>                                                                                                                                                                                                                                                         | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2310.03744)</div>                                                                                                                                  |
|                 |                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                              |

## State Space Model / SSM

| Date    | Title                                                                | Summary                                                                                                                                                                     | Links                                                                                                                                       |
| ------- | -------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| 2023/12 | **Mamba: Linear-Time Sequence Modeling with Selective State Spaces** | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2312.00752)</div> |
|         |                                                                      |                                                                                                                                                                             |                                                                                                                                             |


## Overview

| Date   | Title                                                                           | Summary                                                                                                                                                                                | Links                                                                                                                                       |
| ------ | ------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| 2023/3 | **Foundation Models for Decision Making: Problems, Methods, and Opportunities** | <sub>(1) **Foundation Models as Conditional Generative Models**. (2) **Foundation Models as Representation Learners**. (3) **Large Language Models as Agent and Environments**. </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2303.04129)</div> |
|        |                                                                                 |                                                                                                                                                                                        |                                                                                                                                             |
|        |                                                                                 |                                                                                                                                                                                        |                                                                                                                                             |



## Standard

| Date             | Title                                                                                                          | Summary                                                                                                                                                                     | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ---------------- | -------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 7/4/23<br>ICML23 | **Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback** | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openai/weak-to-strong)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Posts-yellow?logo=rss)](https://mp.weixin.qq.com/s/f6YW-CxnLhnfMWTLg4M4Cw)</div><div style='width:150px;'>[![Note](https://img.shields.io/badge/Note-Read-blue?logo=dependabot)](summary/2024-03/2403.18349.md)</div> |
|                  |                                                                                                                |                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
