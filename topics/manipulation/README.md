# Manipulation


**üìãCatalogue**
* [In-Hand Manipulation](#in-hand-manipulation)


**üî¨Resources**
* [Standard](#standard)
* [KnowledgeBase](./manip_knowledge_base.md)


## Dexterous

| Date         | Title                                                                                       | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Links                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ------------ | ------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 2022<br>CoRL | **In-Hand Object Rotation via Rapid Motor Adaptation**                                      | <sub>Provide a simple **adaptive controller** to achieve object rotation over z-axis. *First*, it trains **a base control policy œÄ** and **a encoder Œº**, with the input of object properties in simulation environment. The policy is trained with PPO. *Second*, in order to deploy, it trains **an adaptation module œÜ** to estimate the extrinsics vector from the discrepancy between the proprioception history and the action history. The module is trained with the trajectories collected from the first step. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§®</sub>                                                                                                                                                                                                                                                                                                                                                                                                         | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2210.04887)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://haozhi.io/hora/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/HaozhiQi/hora/)</div> |
| 2023<br>CoRL | **General In-Hand Object Rotation with Vision and Touch**                                   | <sub>Provide **RotateIt**, a system can rotate objects along **multiple axes**.First train a **Oracle Policy** with ground-truth object properties through **PPO**. Then distill the policy into real-world scenarios with **multimodal data(visual+tactile)**. During the sim2real distilling, train a **Visuotactile Transformer** to infer representation from realistic inputs. üí°Compared with it's former work(*In-Hand Object Rotation via Rapid Motor Adaptation*), 1Ô∏è‚É£**Rotatelt** combines the shape information extracted from mesh(3D-Vision) rather than simply considering the physical properties. And it also adds a rotation penalty to reward function to extend the rotation axises to x and y. 2Ô∏è‚É£**Rotatelt** updates the adaptation module to a Transformer whose inputs are tactile features from sensors and visual features from RGBD cameras, rather than simple proprioception and action histories. üí´\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§î</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2309.09979)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://haozhi.io/rotateit/)</div>                                                                                                                                             |
| 2024         | **Complementarity-Free Multi-Contact Modeling and Optimization for Dexterous Manipulation** |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2408.07855)</div>                                                                                                                                                                                                                                                                              |
|              |                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                          |
|              |                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                          |


## VLA

>Vision Language Action

| Date            | Title                                                                                                           | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| --------------- | --------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 2023<br>RSS     | **RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE** \| RT-1                                          | <sub>Proposed **Robotics Transformer(RT-1)**, a Transformer architecture that takes a history of images and task description as input and directly outputs tokenized actions. **RT-1** flatten the feature map(6 history images) from the **EfficientNet** into **81** visual tokens. To include the language instruction, **RT-1** uses **Universal Sentence Encoder** to embed the instruction and sends this embedding to **identity-initialized FiLM** layers which is added to the pretrained EfficientNet to condition the image encoder. Then **TokenLearner** subsamples the **81** visual tokens that come out of the pre-trained FiLM-EfficientNet layers to just **8** final tokens that are then passed on to **Transformer** layers. Finally, **RT-1** outputs the **action tokens**. ‚≠ê\|üå∏\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2212.06817)</div>                                                                                                                                                                                                                                                                                                                  |
| 2023<br>CoRL    | **RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control** \| RT-2                       | <sub>Present a new **Vision-Language-Action models(VLA)**, **RT-2**, which tries to combine **vision-language models(VLM)** pre-training with robotic data, which takes as input one or more images and produces a sequence of tokens. **RT-2** represents **robot actions** as **another language**, which can be cast into **text tokens** and **trained together with Internet-scale vision-language datasets**. During inference, the text tokens are de-tokenized into robot actions, enabling closed loop control. **RT-2** is like **a fully end-to-end solution**. The action space consists of (1) 6-DoF positional and rotational displacement of the robot end-effector, (2) the level of extension of the robot gripper, (3) a special discrete command for terminating the episode. üí´\|üå∑\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|üòâ </sub>                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2307.15818)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robotics-transformer2.github.io/)</div>                                                                                                                                                                    |
| 2024<br>NeurIPS | **Open X-Embodiment: Robotic Learning Datasets and RT-X Models** \| RT-X                                        | <sub>Provide a **Open X-Embodiment dataset** and test it on the **RT-1** and **RT-2** models. **RT-1-X** is an architecture designed for robotics, with **a FiLM conditioned EfficientNet** and **a Transformer**. **RT-2-X** builds on **a VLM backbone** by representing **actions** as **another language**, and training action text tokens together with  vision-language data. ‚ú®\|üå∏\|‚òÑÔ∏è\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2310.08864)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robotics-transformer-x.github.io/)</div>                                                                                                                                                                   |
| 2024<br>ICRA    | **Octo:¬†An Open-Source Generalist Robot Policy** \| Octo                                                        | <sub>Recently "general-purpose robot models" (e.g., RT-) are limited in three main aspects : (1) they typically constrain downstream users to a pre-defined and often restrictive set of input observations. (2) they lack support for effective fine-tuning to new domains. (3) the largest of these models are not available to the general public. Considering the above problems, **Octo** is proposed. It is **a transformer-based** policy. First, convert the **task definitions(language)** and **observations(visual)** into a "tokenized" format using modality-specific tokenizers. Second, the token sequence is processed by a **block-wise masked** transformer, which enable us to add and remove observations or tasks during fine-tuning. Third, **a readout token**, serving as the [CLS] token in BERT, is sent to the Transformer and is decoded into action by **a lightweight** "action head". Octo allows us to **flexibly** add new task and observation inputs or action output heads to the model during downstream fine-tuning. When inference, **Octo** can only generate **one action** according to **one observation**. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§®</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2405.12213)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://octo-models.github.io/)</div>                                                                                                                                                                              |
| 2024<br>        | **OpenVLA: An Open-Source Vision-Language-Action Model** \| OpenVLA                                             | <sub>Proposed **OpenVLA**, predicting 7-dimensional robot control actions given an image observation and a language instruction. **OpenVLA** consists of three components : (1) a vision encoder that concatenates Dino V2 and SigLIP ; (2) a simple projector ; (3) the LLM backbone, the Prismatic-VLM, which is fine-tuned on Llama2. üí´\|üå∑\|üî•\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2406.09246)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://openvla.github.io/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openvla/openvla)</div>                                 |
| 2024<br>ICML    | **3D-VLA: A 3D Vision-Language-Action Generative World Model** \| 3D-VLA                                        | <sub>Proposed **3D-VLA**, a LLM that links **3D perception**, reasoning and action through **a generative world model**. Specifically, **3D-VLA** uses **a diffusion model** for goal generation, which utilizes RGBD to RGB-D and point-cloud to point-cloud. And then it uses **interactive tokens** to inject the goal generation into **LLM**. üí´\|üå∑\|üî•\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2403.09631)</div>                                                                                                                                                                                                                                                                                                                  |
| 2024<br>ICLR    | **Vision-Language Foundation Models as Effective Robot Imitators** \| RoboFlamingo                              | <sub>Proposed **RobotFlamingo**, which consists of **Flamingo** Vision-Language Model(**a Vision Encoder** plus **a Feature Fusion Decoder**) and **a Policy Head**. Specifically, the Flamingo backbone models single-step observations and the temporal features are modeled by the policy head(LSTM version). In **the Vision Encoder**, it utilizes the attention mechanism to reduce the number of token sequences. In **the Feature Fusion Decoder**, it makes the cross-attention layer take the language token as query, and the encoded visual token as key and value. A relatively simple and small-scale solution. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§®</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2311.01378)</div>                                                                                                                                                                                                                                                                                                                  |
| 2024<br>ICLR    | **Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation** \| GR-1                  | <sub>Proposed **GR-1**, which is pre-trained on the task of language-conditioned video prediction and then is fine-tuned on robot data to learn multi-task visual robot manipulation. GR-1 is a simple **GPT-style** transformer. During fine-tuning, inputs conclude language, visual and **robot state**, which are processed through **CLIP**, **Perceiver Resampler** and **MLP** respectively. For action prediction, GR-1 utilizes **a action decoder** to process action token embedding to get arm(continuous) and gripper(discrete) actions. üí´\|üå∑\|‚ù§Ô∏è‚Äçüî•\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2312.13139)</div>                                                                                                                                                                                                                                                                                                                  |
| 2022<br>RAL     | **What Matters in Language Conditioned Robotic Imitation Learning over Unstructured Data** \| HULC              | <sub>Proposed HULC, trying to learn language conditioned. First, HULC utilizes a **Multimodal Transformer** as **posterior** to encodes sequences of observations to learn high-level behaviors(**latent plan**). It is trained through a **Contrastive Visuo-Lingual Loss** with **instructions**. Then HULC uses a **Plan Sampler Network** to predict **prior** distribution(**latent plan**) through initial state and language goal. Both of the above are trained with **KL divergence**. Finally, HULC constructs a **Local Policy Network** to generate actions after receiving language instructions, observations and **the latent plan**. **HULC** is **language-conditioned** imitation learning method **without LLM**. üí´\|üå∑\|‚òÑÔ∏è\|üëçüèΩ\|üòâ </sub>                                                                                                                                                                                                                                                                                                                                                                                                                 | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2204.06252)</div><br>                                                                                                                                                                                                                                                                                                              |
| 2023<br>ICRA    | **Grounding Language with Visual Affordances over Unstructured Data** \| HULC++                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2210.01911)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](http://hulc2.cs.uni-freiburg.de/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/mees/hulc2)</div>                                |
| 2024            | **RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulaiton** \| RoboUniView | <sub>To solve the significant performance **disparities** across different robotic platforms in recent methods, they proposed **RoboUniView**, which **decouples** visual feature extraction from action learning. **Novelly** they first learn **a unified view representation** from multi-perspective views by pre-training. The Vision Encoder includes a ViT and **UVFormer**(from BEVFormer), which is aimed to get a representation being robust to perspective differences. Then, they utilize a **Feature Fusion Decoder** and a **Policy Head** to generate actions as normal(RoboFlamingo). üí´\|üíê\|üî•\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2406.18977)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/liufanfanlff/RoboUniview)</div>                                                                                                                                                        |
| 2024            | **Robotic Control via Embodied Chain-of-Thought Reasoning** \| ECoT                                             | <sub>Provide **Embodied Chain-of-Thought Reasoning(ECoT)** for VLA. , in which the authors train VLAs to perform multiple steps of **reasoning** about plans, sub-tasks, motions, and visually grounded features like object bounding boxes and end effector positions, **before** predicting the robot action. However, it utilized a number of tricks to downward the computation due to the design of CoT, even though it can improve the success rate compared with **OpenVLA**. ‚ú®\|üíê\|üî•\|üëçüèø\|üòâ </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2407.08693)</div>                                                                                                                                                                                                                                                                                                                  |
| 2024            | **LLaRA: Supercharging Robot Learning Data for Vision-Language Policy** \| LLaRA                                | <sub>It proposed a framework **LLaRA** which converts a set of robot manipulation expert trajectories into **a visuomotor instruction tuning dataset**, then turning a VLM into **a robot policy**. Specifically, it adopts **2D Image Coordinates**‚Äînormalized relative to the image size‚Äîto represent **positional actions**, including a a **rotational** component of the action. And it'll tune a VLM with the above conversations containing 2D coordinates, which be further converted to robot action space via predefined mapping. üí´\|üíê\|üî•\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2406.20095)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/LostXine/LLaRA)</div>                                                                                                                                                                  |
| 2024<br>RSS     | **Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals** \| MDT                  | <sub>This work introduces the **Multimodal Diffusion Transformer (MDT)**, a diffusion-based transformer encoder-decoder architecture, that learns versatile behavior from multimodal goal specifications with few language annotations. 1Ô∏è‚É£ The multimodal transformer encoder first processes the tokens from the current image observations and desired multimodal goals(image or language), converting these inputs into a series of latent representation tokens. Then the decoder functions as a diffuser to process noisy action tokens with **self-attention** and fuse the conditioning information from the **latent state representation** via **cross-attention**. 2Ô∏è‚É£ Two self-supervised auxiliary objectives. One is **MGF** which  reconstructs future states, to include **foresight** into the latent embedding. The other is **CLA** which computes the **InfoNCE** loss using the **cosine similarity** between the image-goal embedding and language-goal embedding, to align visual goals with their language counterparts. ‚≠ê\|üå∏\|‚òÑÔ∏è\|üëçüèª\|ü§®</sub>                                                                                                       | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2407.05996)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://intuitive-robots.github.io/mdt_policy/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/intuitive-robots/mdt_policy)</div> |
| 2024/6          | **LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning** \| LLARVA                                  | <sub>Introduced **LLARVA**, a Large MultiModal Model with instruction tuning method that leverages **structured prompts** and **visual traces**(2-D representations). Specifically, **the instruction template** is featuring the **robot type**, **control mode**, **task instruction**, **proprioceptive information**, and a query indicating the **number of future actions** to predict. To achieve **alignment** between visual inputs and robotic actions, they predict **visual traces** which is **a sequence of coordinates (x, y)** in **a two-dimensional space**,  as an **auxiliary task**. üí´\|üå∑\|‚òÑÔ∏è\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2406.11815)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://llarva24.github.io/)</div>                                                                                                                                                                                 |
| 2024<br>ICLR    | **Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks** \| LCD                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2210.15629)</div>                                                                                                                                                                                                                                                                                                                  |
| 2024<br>ICML    | **Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data** \| SPIL              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2305.19075)</div>                                                                                                                                                                                                                                                                                                                  |
| 2024<br>ICLR    | **Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models** \| SuSIE                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2310.10639)</div>                                                                                                                                                                                                                                                                                                                  |
| 2024/7          | **3D Diffuser Actor: Policy Diffusion with 3D Scene Representations** \| 3D Diffuser Actor                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2402.10885)</div>                                                                                                                                                                                                                                                                                                                  |
|                 |                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
|                 |                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                              |




## Long-Horizon

| Date         | Title                                                                                                          | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | KeyPoints                        | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| ------------ | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2021<br>CoRL | **Adversarial Skill Chaining for Long-Horizon Robot Manipulation via Terminal State Regularization** \| T-STAR | <sub>Proposed **T-STAR**, a **skill-chaining** method to solve long-horizon task. In short, it regularizes the **terminal** state distribution of the **current** subtask to be close to the **initiation** set of the **subsequent** subtask, in order to solve the *never-seen starting state* and *infinite terminal set* problems. 1Ô∏è‚É£Initially, it'll learn the sub-policies with Generative Adversarial Imitation Learning (**GAIL**) method, which utilizes **a discriminator** trained through expert trajectories, to construct a auxiliary reward serving for training process. 2Ô∏è‚É£Then **T-STAR** trains **an initiation set discriminator** to distinguish the terminal sets and the the initial ones. Considering **the set discriminator**, it encourages the sub-policy to reach a **terminal** state close to the **initiation** set of the following sub-policy through **the terminal state regularization**. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèΩ\|ü§®</sub>                                                                                                                                                                           | Skill-Chaining                   | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2111.07999)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://clvrai.github.io/skill-chaining/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/clvrai/skill-chaining)</div>      |
| 2023<br>CoRL | **Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation**                            | <sub>Aiming to solve the **long-horizon** problem for **dexterous hand** manipulation. This paper proposed **a bi-directional optimization** process to obtain sub-policies, utilizing **a Transition Feasibility Function** for **backward-finetuning**. Specifically, 1Ô∏è‚É£**Forward-Initialization Stage**. beyond using PPO to learn each sub-policy as normal, they proposed to sample the initial states of the current sub-policy, from **the successful terminal states** of the prior one. 2Ô∏è‚É£**Backward-Finetuning Stage**. They tried to get a **Transition Feasibility Function** to **map from the a few last states of the prior policy (which are equal to a few initial states of the current policy) to the reward of current policy**. During the backward, the **prior** policy is optimized with **an auxiliary reward component (TFF)** of the **prior - current** policy, in order to receive the message from the following policy. **Moreover**, we can choose **when** and **where** to move on to the next sub-policy according to **a feasible score** composed with the TFF. ‚≠ê\|üå∏\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|ü§î</sub> | Skill-Chaining \| Bi-directional | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2309.00987)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sequential-dexterity.github.io/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/sequential-dexterity/SeqDex)</div> |
| 2023<br>CoRL | MimicPlay: Long-Horizon Imitation Learning by Watching Human Play                                              | <sub>If we want to learn complex tasks from imitation learning with human demonstrations, we need an unattainable amount of data. To reduce the high data requirement, they propose **MINICPLAY**, a **hierarchical** imitation learning algorithm that learns **a high-level planner** from human play and **a low-level control policy** from robot demonstrations. 1Ô∏è‚É£They utilize human play data to learn **a high-level planner** with Gaussian Mixture Model (**GMM**), formalizing the process as **a goal-conditioned 3D trajectory generation** task. 2Ô∏è‚É£They first generate **a latent plan** for **each state** with **the high-level planner**. Then incorporating with **wrist images** and **proprioception data** (robot teleoperation), they train a **transformer-based** low-level policy. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèΩ\|ü§®</sub>                                                                                                                                                                                                                                                                                             | Skill-Chaining                   | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2302.12422)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://mimic-play.github.io/)</div>                                                                                                                                                                        |
| 2023<br>ICRA | **Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks**                  | <sub>The complex environments with **sparse** rewards in **long-horizon** tasks are notably **difficult**. **ICE-LLMs** is proposed to solve the sparse rewards problem by leveraging LLMs as **an assistive intrinsic reward**. Given a state of the environment at a given time, the LLM is tasked with evaluating **the potential intrinsic rewards** of all actions the agent is capable of performing, incorporating the env rewards to form the total reward. Furthermore, this intrinsic reward is **decayed** over time so as to not overemphasise plausible inaccuracies stemming from the LLM. üí´\|üå∑\|üî•\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Intrinsic Reward \| LLM          | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2309.16347)</div>                                                                                                                                                                                                                                                                                                           |
| 2023<br>CoRL | **Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models** \| GSC                        | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Skill-Chaining \| Diffusion      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2401.03360)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://generative-skill-chaining.github.io/)</div>                                                                                                                                                         |
| 2023<br>ICRA | **Plan-Seq-Learn: Language Model Guided RL  <br>for Solving Long Horizon Robotics Tasks**                      | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2405.01534)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://mihdalal.github.io/planseqlearn/)</div>                                                                                                                                                             |
| 2024         | **A Backbone for Long-Horizon Robot Task Understanding** \| TBBF                                               | <sub>To solve the unpredictable outcomes and poor generalization in long-horizon tasks through end-to-end robot learning, it proposed a **Therblig-based Backbone Framework (TBBF)**. First, it utilized a **Meta-RGate SynerFusion Network (MGSF)** to finish **therblig segmentation**, and the therblig is composed of R, TE, D,G,.etc. MGSF is constructed through **LSTM** and **Transformer** sub-networks. Then, it utilized **Action Registration (ActionREG)** to output position and orientation at each point. Specifically, it's identified from expert demonstrations using the **SAM** model and matched to object masks and trajectories. Then it employed therblig-based **YOLOv8** model for task-related object matching. Finally it uses **LLM**-Alignment Policy for **Visual Correction ** and **inverse kinematics** for accurate positioning. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                      | Robotics \| Therblig Labelling   | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://www.arxiv.org/abs/2408.01334)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/therbligsbasedbackbone/home)</div>                                                                                                                                        |
| 2024/8       | **Learn2Decompose: Learning Problem Decomposition for Efficient Task and Motion Planning** \| Learn2Decompose  | <sub>To solve the exponentially longer planning time  required by **TAMP** (Task and Motion Planning) solvers as the planning horizon and the number of objects increase, they proposed **Learn2Decompose**. Specifically, 1Ô∏è‚É£ In offline phase, it generates **subgoals** from demonstrations and trains a GNN model to predict** the importance of objects** during transitions between consecutive subgoals. 2Ô∏è‚É£ In online phase, it generates parallelly **subproblems** based on the predicted importance of objects. The **parallelized hierarchical TAMP** (Task and Motion Planning) runs concurrently to generate subplans for each subproblem. Then these subplans are then concatenated. üí´\|üå∑\|‚òÑÔ∏è\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                      | TAMP \| Subgoals                 | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2408.06843)</div>                                                                                                                                                                                                                                                                                                           |
| 2024<br>ICRA | **Solving Sequential Manipulation Puzzles by Finding Easier Subproblems**                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2405.02053)</div>                                                                                                                                                                                                                                                                                                           |
| 2023<br>ICRA | **STAP: Sequencing Task-Agnostic Policies** \| STAP                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2210.12250)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/stanford.edu/stap/home)</div>                                                                                                                                                      |
|              |                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                       |


## Generalization

| Date        | Title                                                                                              | Summary                                                                                                                                                                      | Links                                                                                                                                                                                                                                                                               |
| ----------- | -------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2024<br>RSS | **THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation** \| COLOSSEUM | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2402.08191)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robot-colosseum.github.io/)</div> |
|             |                                                                                                    |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                     |



## Sim2Real

| Date         | Title                                                                                                | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Links                                                                                                                                                                                                                                                                                           |
| ------------ | ---------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2024<br>RSS  | **Natural Language Can Help Bridge the Sim2Real Gap**                                                | <sub>The key insight of the paper is that if **two image observations** from different domains are labeled with **similar language**, the policy should predict **similar action distributions** for both images. First they pretrained **an image backbone encoder** on cross-domain **language-annotated image data**, during which they designed **two variants** for language supervision, a **simple** one and a **softer** one. Then, they freezed this **encoder** and train **a policy network** and **a policy head **to perform **behavioral cloning (BC)** on action-labeled data from both domains(sim+real). For language labeling, they used froze LLM, and they also use off-the-shelf **miniLM** and **BLEURT** for the **embeddings generation** and **similarity measurement**. üí´\|üå∑\|‚òÑÔ∏è\|üëçüèΩ\|ü§®</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2405.10020)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robin-lab.cs.utexas.edu/lang4sim2real/)</div> |
| 2024<br>ICML | **DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning** \| DecisionNCE | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2402.18137)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://2toinf.github.io/DecisionNCE/)</div>          |
|              |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                 |


## PhySim-based MBRL

| Date                   | Title                                                                                                                                                                     | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Keys          | Links                                                                                                                                                                                                                                                                                             |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2023<br>CoRL<br>(oral) | **Diff-LfD: Contact-aware Model-based Learning from Visual Demonstration for Robotic Manipulation via Differentiable Physics-based Simulation and Rendering** \| Diff-LfD | <sub>Proposed Diff-LfD, which composed of **self-supervised pose and shape estimation** and **contact sequence generation**. 1Ô∏è‚É£ To address the incomplete views, they adopt **a diffusion model** to infer the unseen areas. Then they utilized the **differentiable rendering** to get mesh, textures, and poses over multiple images, through which they optimized optimize the **Diff-SDF**. 2Ô∏è‚É£ **A hierarchical structure consisting** of low-level modules for **contact-point localization**, **accessing** all of the contact **points** through **kinematic** and **stationary** constraint and generating the desired **wrench** ; **contact-force optimization**, which utilizes **gradient-based** methods to optimize ; high-level **contact sequence planning**, through A* search at each node. For **sim2real**, they trained a **network** converting wrench into **joint torques** and the **domain randomization** trick. ‚≠ê\|üå∏\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|ü§®</sub> | contact-aware | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://openreview.net/forum?id=DYPOvNot5F)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/diff-lfd)</div> |
| 2023<br>RSS            | **SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering** \| SAM-RL                                          | <sub>How an **accurate** model can be developed **automatically** and **efficiently** from **raw sensory inputs**? **Sensing-Aware** allows a robot to select **an informative viewpoint** to monitor the task process, in order to **automatically updates** the model by **comparing** rendered images. 1Ô∏è‚É£ **Real2Sim** : initial a model with RDB-D camera through **SAGGI** or **CAD** model, then update a model comparing **the raw visual observation** in **simulation** and the **real** world. 2Ô∏è‚É£ **Learn@Sim** : learn **a Q function** to reflect the **informative viewpoint**, and also learn **an actor** to output the **action in simulation**, both of which are trained with **SL** or **BC**. 3Ô∏è‚É£ **Sim2Real** : train **a residual policy** that takes in the **real** image and the **simulated** action, outputs **the residual action**, to build the gap. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèΩ\|ü§®</sub>                                                             | sensing-aware | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2210.15185)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sites.google.com/view/rss-sam-rl)</div>         |
|                        |                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |               |                                                                                                                                                                                                                                                                                                   |
|                        |                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |               |                                                                                                                                                                                                                                                                                                   |



## Simulation

| Date                 | Title                                                                                                                       | Summary                                                                                                                                                                      | Links                                                                                                                                       |
| -------------------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| 2022<br>ICML<br>wksp | **Differentiable Physics Simulations with Contacts: Do They Have Correct Gradients w.r.t. Position, Velocity and Control?** | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2207.05060)</div> |
| 2023<br>CLDC         | **Improving Gradient Computation for Differentiable Physics Simulation with Contacts**                                      | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2305.00092)</div> |
| 2024<br>IEEE         | **A Review of Differentiable Simulators**                                                                                   | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2407.05560)</div> |
|                      |                                                                                                                             |                                                                                                                                                                              |                                                                                                                                             |
|                      |                                                                                                                             |                                                                                                                                                                              |                                                                                                                                             |



## Dataset

| Date         | Title                                                                                                                       | Summary                                                                                                                                                                                                                                                                                                     | Links                                                                                                                                                                                                                                                                              |
| ------------ | --------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2024<br>ICRA | **RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking** \| MT-ACT | <sub>Provided a multi-task manipulation real-world dataset **RoboSet**, which has 9,500 teleoperation trajectories. **RoboSet** involves 12 skills expressed across multiple tasks and scenes. Moreover, it introduced **Semantic Augmentations** for robotics training process. üí´\|üå∑\|‚òÑÔ∏è\|üëçüèø\|üòâ</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2309.01918)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robopen.github.io/)</div>        |
| 2022<br>RAL  | **CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks** \| CALVIN        | <sub></sub>                                                                                                                                                                                                                                                                                                 | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2112.03227)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](http://calvin.cs.uni-freiburg.de/)</div> |
|              |                                                                                                                             |                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                    |
|              |                                                                                                                             |                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                    |


## Others

| Date         | Title                                                                                                            | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | keywords                      | Links                                                                                                                                                                                                                                                                                          |
| ------------ | ---------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2024<br>ICML | **RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation**             | <sub>Instead of directly using or adapting these models to produce policies or low-level actions, **RoboGen** advocate for a **generative** scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. ‚ú®\|üíê\|üî•\|üëçüèΩ\|üòâ</sub>                                                                                                         | Data-Generation               | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2311.01455)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robogen-ai.github.io/)</div>                 |
| 2024         | **Grasping by Hanging: a Learning-Free Grasping Detection Method for Previously Unseen Objects** \| GbH          | <sub>This paper proposes a **learning-free** **three-stage** method that **predicts grasping poses**, enabling robots to pick up and transfer previously **unseen objects**. The workflow includes **3D scanning**, **hangability detection**, **grasping detection** and **grasping evaluation**.  ‚ú®\|üíê\|üî•\|üëçüèø\|üòâ</sub>                                                                                                                                           | Grasping \| Robotics          | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2408.06734)</div>                                                                                                                                                    |
| 2024<br>IROS | **Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models** | <sub>Proposed **Polaris**, a Synthetic-to-Real pose estimation pipeline for tabletop-level robotic tasks. First, they generate RGB, depth, pose, and observable point clouds, through **3D synthetic data rendering**. Then they interact with **GPT4** to understand the instructions and scene. Finally, they use a pre-trained **Real-world Pose Estimation model** to plan the 6-D manipulation. It's more likely **a robotics program**. ‚ú®\|üíê\|üî•\|üëçüèø\|üòâ</sub> | Tabletop \| Manip \| Robotics | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2408.07975v1)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://star-uu-wang.github.io/Polaris/)</div><br> |
| 2024/8       | **General-purpose Clothes Manipulation with Semantic Keypoints**                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                               | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2408.08160)</div>                                                                                                                                                    |
| 2024         | **RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics**                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                               |                                                                                                                                                                                                                                                                                                |
|              |                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                               |                                                                                                                                                                                                                                                                                                |
|              |                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                               |                                                                                                                                                                                                                                                                                                |



## Standard

| Date             | Title                                                                                                          | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ---------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 7/4/23<br>ICML23 | **Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openai/weak-to-strong)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://mp.weixin.qq.com/s/f6YW-CxnLhnfMWTLg4M4Cw)</div><div style='width:150px;'>[![Note](https://img.shields.io/badge/Note-Read-blue?logo=dependabot)](summary/2024-03/2403.18349.md)</div> |
|                  |                                                                                                                |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
