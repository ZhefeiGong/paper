# Manipulation


**üìãCatalogue**
* [In-Hand Manipulation](#in-hand-manipulation)

**üî¨Resources**
* [Standard](#standard)

**üßÄKnowledge**
* SO(3) : 3D rotation group - Á∫ØÊóãËΩ¨
* SE(3) : Special Euclidean Group - Âàö‰ΩìËøêÂä®

**üìäDataset**
1. **CALVIN**
	* [LeaderBoard](http://calvin.cs.uni-freiburg.de/)
	* [Code](https://github.com/mees/calvin)
	* [Paper](https://arxiv.org/abs/2112.03227)


## In-Hand Manipulation

| Date         | Title                                                     | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Links                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ------------ | --------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 2022<br>CoRL | **In-Hand Object Rotation via Rapid Motor Adaptation**    | <sub>Provide a simple **adaptive controller** to achieve object rotation over z-axis. *First*, it trains **a base control policy œÄ** and **a encoder Œº**, with the input of object properties in simulation environment. The policy is trained with PPO. *Second*, in order to deploy, it trains **an adaptation module œÜ** to estimate the extrinsics vector from the discrepancy between the proprioception history and the action history. The module is trained with the trajectories collected from the first step. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§®</sub>                                                                                                                                                                                                                                                                                                                                                                                                         | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2210.04887)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://haozhi.io/hora/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/HaozhiQi/hora/)</div> |
| 2023<br>CoRL | **General In-Hand Object Rotation with Vision and Touch** | <sub>Provide **RotateIt**, a system can rotate objects along **multiple axes**.First train a **Oracle Policy** with ground-truth object properties through **PPO**. Then distill the policy into real-world scenarios with **multimodal data(visual+tactile)**. During the sim2real distilling, train a **Visuotactile Transformer** to infer representation from realistic inputs. üí°Compared with it's former work(*In-Hand Object Rotation via Rapid Motor Adaptation*), 1Ô∏è‚É£**Rotatelt** combines the shape information extracted from mesh(3D-Vision) rather than simply considering the physical properties. And it also adds a rotation penalty to reward function to extend the rotation axises to x and y. 2Ô∏è‚É£**Rotatelt** updates the adaptation module to a Transformer whose inputs are tactile features from sensors and visual features from RGBD cameras, rather than simple proprioception and action histories. üí´\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§î</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2309.09979)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://haozhi.io/rotateit/)</div>                                                                                                                                             |
|              |                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                          |


## VLA / Vision Language Action Model

| Date            | Title                                                                                  | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Links                                                                                                                                                                                                                                                                                                                                                                                                                        |
| --------------- | -------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2023<br>CoRL    | **RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control**      | <sub>Present a new **Vision-Language-Action models(VLA)**, **RT-2**, which tries to combine **vision-language models(VLM)** pre-training with robotic data. RT-2 represents robot actions as another language, which can be cast into text tokens and trained together with Internet-scale vision-language datasets. During inference, the text tokens are de-tokenized into robot actions, enabling closed loop control. RT-2 is like **a fully end-to-end solution**. The action space consists of (1) 6-DoF positional and rotational displacement of the robot end-effector, (2) the level of extension of the robot gripper, (3) a special discrete command for terminating the episode. üí´\|üå∑\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|üòâ </sub>                                                                                                                                                                                                                                                                                                                                     | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2307.15818)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robotics-transformer2.github.io/)</div>                                                                                                                                    |
| 2024<br>NeurIPS | **Open X-Embodiment: Robotic Learning Datasets and RT-X Models**                       | <sub>Provide a **Open X-Embodiment dataset** and test it on the **RT-1** and **RT-2** models. **RT-1-X** is an architecture designed for robotics, with a FiLM conditioned EfficientNet and a Transformer. **RT-2-X** builds on a VLM backbone by representing actions as another language, and training action text tokens together with  vision-language data. ‚ú®\|üå∏\|‚òÑÔ∏è\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2310.08864)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robotics-transformer-x.github.io/)</div>                                                                                                                                   |
| 2023<br>NeurIPS | **Visual Instruction Tuning**                                                          | <sub>Propose **LLaVA** (Large Language and Vision Assistant), an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general purpose visual and language understanding. It's a **VLM** built upon LLaMA, Vicuna, and CLIP. In a nutshell, LLaVA uses a pre-trained CLIP visual encoder to process the input image into image features. Then it uses a trainable projection matrix  to convert the features into language embedding tokens. Thus, for each image, LLaVA generates multi-turn conversation data to instruction-tuning the LLM. ‚≠ê\|üå∏\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|ü§®</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                    | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2304.08485)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://llava-vl.github.io/)</div>                                                                                                                                                 |
| 2024<br>CVPR    | **Improved Baselines with Visual Instruction Tuning**                                  | <sub>Proposed **LLaVA-1.5**. With simple modifications to LLaVA, namely, using **CLIP-ViT-L-336px with an MLP projection** and **adding academic-task-oriented VQA data with response formatting prompts**, LLaVA-1.5 established stronger baselines that achieve state-of-the-art across 11 benchmarks. ‚ú®\|üíê\|üî•\|üëçüèø\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2310.03744)</div>                                                                                                                                                                                                                                                                                  |
| 2024<br>ICRA    | **Octo:¬†An Open-Source Generalist Robot Policy**                                       | <sub>Recently "general-purpose robot models" (e.g., RT-) are limited in three main aspects : (1) they typically constrain downstream users to a pre-defined and often restrictive set of input observations. (2) they lack support for effective fine-tuning to new domains. (3) the largest of these models are not available to the general public. Considering the above problems, **Octo** is proposed. It is a transformer-based policy. First, convert the task definitions(language) and observations(visual) into a "tokenized" format using modality-specific tokenizers. Second, the token sequence is processed by a **block-wise masked** transformer, which enable us to add and remove observations or tasks during fine-tuning. Third, a readout token, serving as the [CLS] token in BERT, is sent to the Transformer and is decoded into action by a lightweight "action head". Octo allows us to **flexibly** add new task and observation inputs or action output heads to the model during downstream fine-tuning. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§®</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2405.12213)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://octo-models.github.io/)</div>                                                                                                                                              |
| 2024<br>ICLR    | **Vision-Language Foundation Models as Effective Robot Imitators**                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2311.01378)</div>                                                                                                                                                                                                                                                                                  |
| 2024<br>ICLR    | **Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation** |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2312.13139)</div>                                                                                                                                                                                                                                                                                  |
| 2024<br>        | **OpenVLA: An Open-Source Vision-Language-Action Model**                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2406.09246)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://openvla.github.io/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openvla/openvla)</div> |
|                 |                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                              |
|                 |                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                              |


## LLM / Large Language Model

| Date         | Title                                                                                                | Summary                                                                                                                                                                                                                                                                                                                                                     | Links                                                                                                                                                                                                                                                                          |
| ------------ | ---------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 2024<br>ICML | **RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation** | <sub>Instead of directly using or adapting these models to produce policies or low-level actions, **RoboGen** advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. ‚ú®\|üíê\|üî•\|üëçüèΩ\|üòâ</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2311.01455)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robogen-ai.github.io/)</div> |
|              |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                |





## Standard

| Date             | Title                                                                                                          | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ---------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 7/4/23<br>ICML23 | **Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openai/weak-to-strong)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://mp.weixin.qq.com/s/f6YW-CxnLhnfMWTLg4M4Cw)</div><div style='width:150px;'>[![Note](https://img.shields.io/badge/Note-Read-blue?logo=dependabot)](summary/2024-03/2403.18349.md)</div> |
|                  |                                                                                                                |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
