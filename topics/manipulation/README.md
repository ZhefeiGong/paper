# Manipulation


**üìãCatalogue**
* [In-Hand Manipulation](#in-hand-manipulation)

**üî¨Resources**
* [Standard](#standard)

**üßÄKnowledge**
* SO(3) : 3D rotation group - Á∫ØÊóãËΩ¨
* SE(3) : Special Euclidean Group - Âàö‰ΩìËøêÂä®

**üìäDataset**
1. **CALVIN**
	* [LeaderBoard](http://calvin.cs.uni-freiburg.de/)
	* [Code](https://github.com/mees/calvin)
	* [Paper](https://arxiv.org/abs/2112.03227)


## In-Hand Manipulation

| Date         | Title                                                     | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Links                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ------------ | --------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 2022<br>CoRL | **In-Hand Object Rotation via Rapid Motor Adaptation**    | <sub>Provide a simple **adaptive controller** to achieve object rotation over z-axis. *First*, it trains **a base control policy œÄ** and **a encoder Œº**, with the input of object properties in simulation environment. The policy is trained with PPO. *Second*, in order to deploy, it trains **an adaptation module œÜ** to estimate the extrinsics vector from the discrepancy between the proprioception history and the action history. The module is trained with the trajectories collected from the first step. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§®</sub>                                                                                                                                                                                                                                                                                                                                                                                                         | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2210.04887)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://haozhi.io/hora/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/HaozhiQi/hora/)</div> |
| 2023<br>CoRL | **General In-Hand Object Rotation with Vision and Touch** | <sub>Provide **RotateIt**, a system can rotate objects along **multiple axes**.First train a **Oracle Policy** with ground-truth object properties through **PPO**. Then distill the policy into real-world scenarios with **multimodal data(visual+tactile)**. During the sim2real distilling, train a **Visuotactile Transformer** to infer representation from realistic inputs. üí°Compared with it's former work(*In-Hand Object Rotation via Rapid Motor Adaptation*), 1Ô∏è‚É£**Rotatelt** combines the shape information extracted from mesh(3D-Vision) rather than simply considering the physical properties. And it also adds a rotation penalty to reward function to extend the rotation axises to x and y. 2Ô∏è‚É£**Rotatelt** updates the adaptation module to a Transformer whose inputs are tactile features from sensors and visual features from RGBD cameras, rather than simple proprioception and action histories. üí´\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§î</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2309.09979)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://haozhi.io/rotateit/)</div>                                                                                                                                             |
|              |                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                          |

## Vision Language Action / VLA

| Date            | Title                                                                                      | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Links                                                                                                                                                                                                                                                                                                                                                                                                                        |     |
| --------------- | ------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --- |
| 2023<br>RSS     | **RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE** \| RT-1                     | <sub>Proposed **Robotics Transformer(RT-1)**, a Transformer architecture that takes a history of images and task description as input and directly outputs tokenized actions. **RT-1** flatten the feature map(6 history images) from the **EfficientNet** into **81** visual tokens. To include the language instruction, **RT-1** uses **Universal Sentence Encoder** to embed the instruction and sends this embedding to **identity-initialized FiLM** layers which is added to the pretrained EfficientNet to condition the image encoder. Then **TokenLearner** subsamples the **81** visual tokens that come out of the pre-trained FiLM-EfficientNet layers to just **8** final tokens that are then passed on to **Transformer** layers. Finally, **RT-1** outputs the **action tokens**. ‚≠ê\|üå∏\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2212.06817)</div>                                                                                                                                                                                                                                                                                  |     |
| 2023<br>CoRL    | **RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control** \| RT-2  | <sub>Present a new **Vision-Language-Action models(VLA)**, **RT-2**, which tries to combine **vision-language models(VLM)** pre-training with robotic data, which takes as input one or more images and produces a sequence of tokens. **RT-2** represents **robot actions** as **another language**, which can be cast into **text tokens** and **trained together with Internet-scale vision-language datasets**. During inference, the text tokens are de-tokenized into robot actions, enabling closed loop control. **RT-2** is like **a fully end-to-end solution**. The action space consists of (1) 6-DoF positional and rotational displacement of the robot end-effector, (2) the level of extension of the robot gripper, (3) a special discrete command for terminating the episode. üí´\|üå∑\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|üòâ </sub>                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2307.15818)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robotics-transformer2.github.io/)</div>                                                                                                                                    |     |
| 2024<br>NeurIPS | **Open X-Embodiment: Robotic Learning Datasets and RT-X Models** \| RT-X                   | <sub>Provide a **Open X-Embodiment dataset** and test it on the **RT-1** and **RT-2** models. **RT-1-X** is an architecture designed for robotics, with **a FiLM conditioned EfficientNet** and **a Transformer**. **RT-2-X** builds on **a VLM backbone** by representing **actions** as **another language**, and training action text tokens together with  vision-language data. ‚ú®\|üå∏\|‚òÑÔ∏è\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2310.08864)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robotics-transformer-x.github.io/)</div>                                                                                                                                   |     |
| 2024<br>ICRA    | **Octo:¬†An Open-Source Generalist Robot Policy** \| Octo                                   | <sub>Recently "general-purpose robot models" (e.g., RT-) are limited in three main aspects : (1) they typically constrain downstream users to a pre-defined and often restrictive set of input observations. (2) they lack support for effective fine-tuning to new domains. (3) the largest of these models are not available to the general public. Considering the above problems, **Octo** is proposed. It is **a transformer-based** policy. First, convert the **task definitions(language)** and **observations(visual)** into a "tokenized" format using modality-specific tokenizers. Second, the token sequence is processed by a **block-wise masked** transformer, which enable us to add and remove observations or tasks during fine-tuning. Third, **a readout token**, serving as the [CLS] token in BERT, is sent to the Transformer and is decoded into action by **a lightweight** "action head". Octo allows us to **flexibly** add new task and observation inputs or action output heads to the model during downstream fine-tuning. When inference, **Octo** can only generate **one action** according to **one observation**. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§®</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2405.12213)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://octo-models.github.io/)</div>                                                                                                                                              |     |
| 2024<br>ICLR    | **Vision-Language Foundation Models as Effective Robot Imitators**                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2311.01378)</div>                                                                                                                                                                                                                                                                                  |     |
| 2024<br>ICLR    | **Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation**     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2312.13139)</div>                                                                                                                                                                                                                                                                                  |     |
| 2022<br>RAL     | **What Matters in Language Conditioned Robotic Imitation Learning over Unstructured Data** |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2204.06252)</div><br>                                                                                                                                                                                                                                                                              |     |
| 2024<br>        | **OpenVLA: An Open-Source Vision-Language-Action Model** \| OpenVLA                        | <sub>Proposed **OpenVLA**, predicting 7-dimensional robot control actions given an image observation and a language instruction. **OpenVLA** consists of three components : (1) a vision encoder that concatenates Dino V2 and SigLIP ; (2) a simple projector ; (3) the LLM backbone, the Prismatic-VLM, which is fine-tuned on Llama2. üí´\|üå∑\|üî•\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2406.09246)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://openvla.github.io/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openvla/openvla)</div> |     |
| 2024<br>ICML    | **3D-VLA: A 3D Vision-Language-Action Generative World Model**                             | <sub>Proposed **3D-VLA**, a LLM that links **3D perception**, reasoning and action through **a generative world model**. Specifically, **3D-VLA** uses **a diffusion model** for goal generation, which utilizes RGBD to RGB-D and point-cloud to point-cloud. And then it uses **interactive tokens** to inject the goal generation into LLM. üí´\|üå∑\|üî•\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2403.09631)</div>                                                                                                                                                                                                                                                                                  |     |
|                 |                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                              |     |
|                 |                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                              |     |



## LLM / Large Language Model

| Date         | Title                                                                                                | Summary                                                                                                                                                                                                                                                                                                                                                         | Links                                                                                                                                                                                                                                                                          |
| ------------ | ---------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 2024<br>ICML | **RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation** | <sub>Instead of directly using or adapting these models to produce policies or low-level actions, **RoboGen** advocate for a **generative** scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. ‚ú®\|üíê\|üî•\|üëçüèΩ\|üòâ</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2311.01455)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robogen-ai.github.io/)</div> |
|              |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                |
|              |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                |





## Standard

| Date             | Title                                                                                                          | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ---------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 7/4/23<br>ICML23 | **Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openai/weak-to-strong)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://mp.weixin.qq.com/s/f6YW-CxnLhnfMWTLg4M4Cw)</div><div style='width:150px;'>[![Note](https://img.shields.io/badge/Note-Read-blue?logo=dependabot)](summary/2024-03/2403.18349.md)</div> |
|                  |                                                                                                                |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
