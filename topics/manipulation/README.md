# Manipulation


**üìãCatalogue**
* [In-Hand Manipulation](#in-hand-manipulation)

**üî¨Resources**
* [Standard](#standard)

## Dexterous Hand

| Date         | Title                                                     | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Links                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ------------ | --------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 2022<br>CoRL | **In-Hand Object Rotation via Rapid Motor Adaptation**    | <sub>Provide a simple **adaptive controller** to achieve object rotation over z-axis. *First*, it trains **a base control policy œÄ** and **a encoder Œº**, with the input of object properties in simulation environment. The policy is trained with PPO. *Second*, in order to deploy, it trains **an adaptation module œÜ** to estimate the extrinsics vector from the discrepancy between the proprioception history and the action history. The module is trained with the trajectories collected from the first step. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§®</sub>                                                                                                                                                                                                                                                                                                                                                                                                         | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2210.04887)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://haozhi.io/hora/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/HaozhiQi/hora/)</div> |
| 2023<br>CoRL | **General In-Hand Object Rotation with Vision and Touch** | <sub>Provide **RotateIt**, a system can rotate objects along **multiple axes**.First train a **Oracle Policy** with ground-truth object properties through **PPO**. Then distill the policy into real-world scenarios with **multimodal data(visual+tactile)**. During the sim2real distilling, train a **Visuotactile Transformer** to infer representation from realistic inputs. üí°Compared with it's former work(*In-Hand Object Rotation via Rapid Motor Adaptation*), 1Ô∏è‚É£**Rotatelt** combines the shape information extracted from mesh(3D-Vision) rather than simply considering the physical properties. And it also adds a rotation penalty to reward function to extend the rotation axises to x and y. 2Ô∏è‚É£**Rotatelt** updates the adaptation module to a Transformer whose inputs are tactile features from sensors and visual features from RGBD cameras, rather than simple proprioception and action histories. üí´\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§î</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2309.09979)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://haozhi.io/rotateit/)</div>                                                                                                                                             |
|              |                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                          |



## VLA / Vision Language Action

| Date            | Title                                                                                                           | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Links                                                                                                                                                                                                                                                                                                                                                                                                                        |
| --------------- | --------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2023<br>RSS     | **RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE** \| RT-1                                          | <sub>Proposed **Robotics Transformer(RT-1)**, a Transformer architecture that takes a history of images and task description as input and directly outputs tokenized actions. **RT-1** flatten the feature map(6 history images) from the **EfficientNet** into **81** visual tokens. To include the language instruction, **RT-1** uses **Universal Sentence Encoder** to embed the instruction and sends this embedding to **identity-initialized FiLM** layers which is added to the pretrained EfficientNet to condition the image encoder. Then **TokenLearner** subsamples the **81** visual tokens that come out of the pre-trained FiLM-EfficientNet layers to just **8** final tokens that are then passed on to **Transformer** layers. Finally, **RT-1** outputs the **action tokens**. ‚≠ê\|üå∏\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2212.06817)</div>                                                                                                                                                                                                                                                                                  |
| 2023<br>CoRL    | **RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control** \| RT-2                       | <sub>Present a new **Vision-Language-Action models(VLA)**, **RT-2**, which tries to combine **vision-language models(VLM)** pre-training with robotic data, which takes as input one or more images and produces a sequence of tokens. **RT-2** represents **robot actions** as **another language**, which can be cast into **text tokens** and **trained together with Internet-scale vision-language datasets**. During inference, the text tokens are de-tokenized into robot actions, enabling closed loop control. **RT-2** is like **a fully end-to-end solution**. The action space consists of (1) 6-DoF positional and rotational displacement of the robot end-effector, (2) the level of extension of the robot gripper, (3) a special discrete command for terminating the episode. üí´\|üå∑\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|üòâ </sub>                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2307.15818)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robotics-transformer2.github.io/)</div>                                                                                                                                    |
| 2024<br>NeurIPS | **Open X-Embodiment: Robotic Learning Datasets and RT-X Models** \| RT-X                                        | <sub>Provide a **Open X-Embodiment dataset** and test it on the **RT-1** and **RT-2** models. **RT-1-X** is an architecture designed for robotics, with **a FiLM conditioned EfficientNet** and **a Transformer**. **RT-2-X** builds on **a VLM backbone** by representing **actions** as **another language**, and training action text tokens together with  vision-language data. ‚ú®\|üå∏\|‚òÑÔ∏è\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2310.08864)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robotics-transformer-x.github.io/)</div>                                                                                                                                   |
| 2024<br>ICRA    | **Octo:¬†An Open-Source Generalist Robot Policy** \| Octo                                                        | <sub>Recently "general-purpose robot models" (e.g., RT-) are limited in three main aspects : (1) they typically constrain downstream users to a pre-defined and often restrictive set of input observations. (2) they lack support for effective fine-tuning to new domains. (3) the largest of these models are not available to the general public. Considering the above problems, **Octo** is proposed. It is **a transformer-based** policy. First, convert the **task definitions(language)** and **observations(visual)** into a "tokenized" format using modality-specific tokenizers. Second, the token sequence is processed by a **block-wise masked** transformer, which enable us to add and remove observations or tasks during fine-tuning. Third, **a readout token**, serving as the [CLS] token in BERT, is sent to the Transformer and is decoded into action by **a lightweight** "action head". Octo allows us to **flexibly** add new task and observation inputs or action output heads to the model during downstream fine-tuning. When inference, **Octo** can only generate **one action** according to **one observation**. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§®</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2405.12213)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://octo-models.github.io/)</div>                                                                                                                                              |
| 2024<br>        | **OpenVLA: An Open-Source Vision-Language-Action Model** \| OpenVLA                                             | <sub>Proposed **OpenVLA**, predicting 7-dimensional robot control actions given an image observation and a language instruction. **OpenVLA** consists of three components : (1) a vision encoder that concatenates Dino V2 and SigLIP ; (2) a simple projector ; (3) the LLM backbone, the Prismatic-VLM, which is fine-tuned on Llama2. üí´\|üå∑\|üî•\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2406.09246)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://openvla.github.io/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openvla/openvla)</div> |
| 2024<br>ICML    | **3D-VLA: A 3D Vision-Language-Action Generative World Model**                                                  | <sub>Proposed **3D-VLA**, a LLM that links **3D perception**, reasoning and action through **a generative world model**. Specifically, **3D-VLA** uses **a diffusion model** for goal generation, which utilizes RGBD to RGB-D and point-cloud to point-cloud. And then it uses **interactive tokens** to inject the goal generation into **LLM**. üí´\|üå∑\|üî•\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2403.09631)</div>                                                                                                                                                                                                                                                                                  |
| 2024<br>ICLR    | **Vision-Language Foundation Models as Effective Robot Imitators** \| RobotFlamingo                             | <sub>Proposed **RobotFlamingo**, which consists of **Flamingo** Vision-Language Model(**a Vision Encoder** plus **a Feature Fusion Decoder**) and **a Policy Head**. Specifically, the Flamingo backbone models single-step observations and the temporal features are modeled by the policy head(LSTM version). In the Vision Encoder, it utilizes the attention mechanism to reduce the number of token sequences. In the Feature Fusion Decoder, it makes the cross-attention layer take the language token as query, and the encoded visual token as key and value. A relatively simple and small-scale solution. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèª\|ü§®</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2311.01378)</div>                                                                                                                                                                                                                                                                                  |
| 2024<br>ICLR    | **Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation** \| GR-1                  | <sub>Proposed **GR-1**, which is pre-trained on the task of language-conditioned video prediction and then is fine-tuned on robot data to learn multi-task visual robot manipulation. GR-1 is a simple **GPT-style** transformer. During fine-tuning, inputs conclude language, visual and **robot state**, which are processed through **CLIP**, **Perceiver Resampler** and **MLP** respectively. For action prediction, GR-1 utilizes **a action decoder** to process action token embedding to get arm(continuous) and gripper(discrete) actions. üí´\|üå∑\|‚ù§Ô∏è‚Äçüî•\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2312.13139)</div>                                                                                                                                                                                                                                                                                  |
| 2022<br>RAL     | **What Matters in Language Conditioned Robotic Imitation Learning over Unstructured Data** \| HULC              | <sub>Proposed HULC, trying to learn language conditioned. First, HULC utilizes a **Multimodal Transformer** as **posterior** to encodes sequences of observations to learn high-level behaviors(**latent plan**). It is trained through a **Contrastive Visuo-Lingual Loss** with instructions. Then HULC uses a **Plan Sampler Network** to predict **prior** distribution(**latent plan**) through initial state and language goal. Both of the above are trained with **KL divergence**. Finally, HULC constructs a **Local Policy Network** to generate actions after receiving language instructions, observations and **the latent plan**. **HULC** is language-conditioned imitation learning method without LLM. üí´\|üå∑\|‚òÑÔ∏è\|üëçüèΩ\|üòâ </sub>                                                                                                                                                                                                                                                                                                                                                                                                                             | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2204.06252)</div><br>                                                                                                                                                                                                                                                                              |
| 2024            | **RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulaiton** \| RoboUniView |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2406.18977)</div>                                                                                                                                                                                                                                                                                  |
| 2024            | Robotic Control via Embodied Chain-of-Thought Reasoning \| ECoT                                                 | <sub>Provide **Embodied Chain-of-Thought Reasoning(ECoT)** for VLA. , in which the authors train VLAs to perform multiple steps of reasoning about plans, sub-tasks, motions, and visually grounded features like object bounding boxes and end effector positions, before predicting the robot action. However, it utilized a number of tricks to download the computation due to the design of CoT, even though it can improve the success rate compared with **OpenVLA**. ‚ú®\|üíê\|üî•\|üëçüèø\|üòâ </sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2407.08693)</div>                                                                                                                                                                                                                                                                                  |
|                 |                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                              |



## LLM / Large Language Model

| Date         | Title                                                                                                | Summary                                                                                                                                                                                                                                                                                                                                                         | Links                                                                                                                                                                                                                                                                          |
| ------------ | ---------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 2024<br>ICML | **RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation** | <sub>Instead of directly using or adapting these models to produce policies or low-level actions, **RoboGen** advocate for a **generative** scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. ‚ú®\|üíê\|üî•\|üëçüèΩ\|üòâ</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2311.01455)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robogen-ai.github.io/)</div> |
|              |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                |
|              |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                |


## Long-Horizon 

| Date         | Title                                                                                                          | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | KeyPoints                        | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| ------------ | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2021<br>CoRL | **Adversarial Skill Chaining for Long-Horizon Robot Manipulation via Terminal State Regularization** \| T-STAR | <sub>Proposed **T-STAR**, a **skill-chaining** method to solve long-horizon task. In short, it regularizes the **terminal** state distribution of the **current** subtask to be close to the **initiation** set of the **subsequent** subtask, in order to solve the *never-seen starting state* and *infinite terminal set* problems. 1Ô∏è‚É£Initially, it'll learn the sub-policies with Generative Adversarial Imitation Learning (**GAIL**) method, which utilizes **a discriminator** trained through expert trajectories, to construct a auxiliary reward serving for training process. 2Ô∏è‚É£Then **T-STAR** trains **an initiation set discriminator** to distinguish the terminal sets and the the initial ones. Considering **the set discriminator**, it encourages the sub-policy to reach a **terminal** state close to the **initiation** set of the following sub-policy through **the terminal state regularization**. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèΩ\|ü§®</sub>                                                                                                                                                                           | Skill-Chaining                   | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2111.07999)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://clvrai.github.io/skill-chaining/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/clvrai/skill-chaining)</div>      |
| 2023<br>CoRL | **Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation**                            | <sub>Aiming to solve the **long-horizon** problem for **dexterous hand** manipulation. This paper proposed **a bi-directional optimization** process to obtain sub-policies, utilizing **a Transition Feasibility Function** for **backward-finetuning**. Specifically, 1Ô∏è‚É£**Forward-Initialization Stage**. beyond using PPO to learn each sub-policy as normal, they proposed to sample the initial states of the current sub-policy, from **the successful terminal states** of the prior one. 2Ô∏è‚É£**Backward-Finetuning Stage**. They tried to get a **Transition Feasibility Function** to **map from the a few last states of the prior policy (which are equal to a few initial states of the current policy) to the reward of current policy**. During the backward, the **prior** policy is optimized with **an auxiliary reward component (TFF)** of the **prior - current** policy, in order to receive the message from the following policy. **Moreover**, we can choose **when** and **where** to move on to the next sub-policy according to **a feasible score** composed with the TFF. ‚≠ê\|üå∏\|‚ù§Ô∏è‚Äçüî•\|üëçüèª\|ü§î</sub> | Skill-Chaining \| Bi-directional | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2309.00987)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://sequential-dexterity.github.io/)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/sequential-dexterity/SeqDex)</div> |
| 2023<br>CoRL | MimicPlay: Long-Horizon Imitation Learning by Watching Human Play                                              | <sub>If we want to learn complex tasks from imitation learning with human demonstrations, we need an unattainable amount of data. To reduce the high data requirement, they propose **MINICPLAY**, a **hierarchical** imitation learning algorithm that learns **a high-level planner** from human play and **a low-level control policy** from robot demonstrations. 1Ô∏è‚É£They utilize human play data to learn **a high-level planner** with Gaussian Mixture Model (**GMM**), formalizing the process as **a goal-conditioned 3D trajectory generation** task. 2Ô∏è‚É£They first generate **a latent plan** for **each state** with **the high-level planner**. Then incorporating with **wrist images** and **proprioception data** (robot teleoperation), they train a **transformer-based** low-level policy. ‚≠ê\|üå∑\|‚òÑÔ∏è\|üëçüèΩ\|ü§®</sub>                                                                                                                                                                                                                                                                                             | Skill-Chaining                   | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2302.12422)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://mimic-play.github.io/)</div>                                                                                                                                                                        |
| 2023<br>ICRA | **Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks**                  | <sub>The complex environments with **sparse** rewards in **long-horizon** tasks are notably **difficult**. **ICE-LLMs** is proposed to solve the sparse rewards problem by leveraging LLMs as **an assistive intrinsic reward**. Given a state of the environment at a given time, the LLM is tasked with evaluating **the potential intrinsic rewards** of all actions the agent is capable of performing, incorporating the env rewards to form the total reward. Furthermore, this intrinsic reward is **decayed** over time so as to not overemphasise plausible inaccuracies stemming from the LLM. üí´\|üå∑\|üî•\|üëçüèΩ\|üòâ</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Intrinsic Reward \| LLM          | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2309.16347)</div>                                                                                                                                                                                                                                                                                                           |
| 2023<br>CoRL | **Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models**                               | <sub>Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic</sub>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Skill-Chaining \| Diffusion      | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2401.03360)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://generative-skill-chaining.github.io/)</div>                                                                                                                                                         |
| 2023<br>ICRA | **Plan-Seq-Learn: Language Model Guided RL  <br>for Solving Long Horizon Robotics Tasks**                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                  | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2405.01534)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://mihdalal.github.io/planseqlearn/)</div>                                                                                                                                                             |
|              |                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|              |                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                       |



## Dataset
| Date         | Title                                                                                                             | Summary                                                                                                                                                                                                                                                                                                     | Links                                                                                                                                                                                                                                                                       |
| ------------ | ----------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2024<br>ICRA | **RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking** | <sub>Provided a multi-task manipulation real-world dataset **RoboSet**, which has 9,500 teleoperation trajectories. **RoboSet** involves 12 skills expressed across multiple tasks and scenes. Moreover, it introduced **Semantic Augmentations** for robotics training process. üí´\|üå∑\|‚òÑÔ∏è\|üëçüèø\|üòâ</sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/abs/2309.01918)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://robopen.github.io/)</div> |
|              |                                                                                                                   |                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                             |
|              |                                                                                                                   |                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                             |


## Standard

| Date             | Title                                                                                                          | Summary                                                                                                                                                                       | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ---------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 7/4/23<br>ICML23 | **Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback** | <sub> Unifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic MotivationUnifying Count-Based Exploration and Intrinsic </sub> | <div style='width:150px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)</div><div style='width:150px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openai/weak-to-strong)</div><div style='width:150px;'>[![Blog](https://img.shields.io/badge/Blog-Website-yellow?logo=rss)](https://mp.weixin.qq.com/s/f6YW-CxnLhnfMWTLg4M4Cw)</div><div style='width:150px;'>[![Note](https://img.shields.io/badge/Note-Read-blue?logo=dependabot)](summary/2024-03/2403.18349.md)</div> |
|                  |                                                                                                                |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
